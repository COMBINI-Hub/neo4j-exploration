{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Headers for original predication and predication aux files. \n",
    "predication_headers = [\n",
    "    'PREDICATION_ID', 'SENTENCE_ID', 'PMID', 'PREDICATE', 'SUBJECT_CUI',\n",
    "    'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY', 'OBJECT_CUI',\n",
    "    'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY', 'FACT_VALUE_CHAR',\n",
    "    'MOD_SCALE_CHAR', 'MOD_VALUE_FLOAT'\n",
    "]\n",
    "\n",
    "predication_aux_headers = [\n",
    "    'PREDICATION_AUX_ID', 'PREDICATION_ID', 'SUBJECT_TEXT', 'SUBJECT_DIST',\n",
    "    'SUBJECT_MAXDIST', 'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE',\n",
    "    'INDICATOR_TYPE', 'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX', 'OBJECT_TEXT',\n",
    "    'OBJECT_DIST', 'OBJECT_MAXDIST', 'OBJECT_START_INDEX', 'OBJECT_END_INDEX',\n",
    "    'OBJECT_SCORE', 'CURR_TIMESTAMP'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Nodes\n",
    "The following code will create two CSVs for entity and predication nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original CSV files\n",
    "df = pd.read_csv('semmed_data/predication.csv', names=predication_headers, encoding='ISO-8859-1', on_bad_lines='warn', na_values=['\\\\N'])\n",
    "\n",
    "df_aux = pd.read_csv('semmed_data/predication_aux.csv', names=predication_aux_headers, encoding='ISO-8859-1', on_bad_lines='warn', na_values=['\\\\N']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, df_aux, on='PREDICATION_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predication_df from both main and aux dataframes\n",
    "predication_base_columns = ['PREDICATION_ID', 'SENTENCE_ID', 'PMID', 'PREDICATE',\n",
    "                          'SUBJECT_CUI', 'OBJECT_CUI']\n",
    "predication_aux_columns = ['PREDICATION_ID', 'INDICATOR_TYPE', \n",
    "                         'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX']\n",
    "\n",
    "# Get base predication info\n",
    "predication_df = df[predication_base_columns].copy()\n",
    "\n",
    "# Get auxiliary info and merge\n",
    "aux_info = df_aux[predication_aux_columns].copy()\n",
    "predication_df = predication_df.merge(aux_info, on='PREDICATION_ID', how='left')\n",
    "aux_info = aux_info.rename(columns={'PREDICATION_ID': 'PREDICATION_ID:ID'})\n",
    "predication_df = predication_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predication_df to CSV with each field in quotes\n",
    "predication_df.to_csv('data/predication.csv', index=False, quoting=csv.QUOTE_ALL, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create concept_df from main and aux dataframes\n",
    "subject_base_columns = ['PREDICATION_ID', 'SUBJECT_CUI', 'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY']\n",
    "subject_base = df[subject_base_columns].copy()\n",
    "\n",
    "# Subject columns from aux df\n",
    "subject_aux_columns = ['PREDICATION_ID', 'SUBJECT_TEXT', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', \n",
    "                      'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE']\n",
    "subject_aux = df_aux[subject_aux_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object columns from predication df\n",
    "object_base_columns = ['PREDICATION_ID', 'OBJECT_CUI', 'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY']\n",
    "object_base = df[object_base_columns].copy()\n",
    "\n",
    "# Object columns from aux df\n",
    "object_aux_columns = ['PREDICATION_ID', 'OBJECT_TEXT', 'OBJECT_DIST', 'OBJECT_MAXDIST', \n",
    "                     'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE']\n",
    "object_aux = df_aux[object_aux_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge base and aux for subjects and objects\n",
    "subject_entities = subject_base.merge(subject_aux, on='PREDICATION_ID').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_entities = object_base.merge(object_aux, on='PREDICATION_ID').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to prepare for merging\n",
    "concept_columns = ['CUI:ID', 'NAME', 'SEMTYPE', 'NOVELTY', 'TEXT', \n",
    "                  'DIST', 'MAXDIST', 'START_INDEX', 'END_INDEX', 'SCORE']\n",
    "subject_entities.columns = concept_columns\n",
    "object_entities.columns = concept_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine subject and object entities and remove duplicates based on CUI\n",
    "concept_df = pd.concat([subject_entities, object_entities]).drop_duplicates(subset=['CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = concept_df.drop(columns=['PREDICATION_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_columns = ['SUBJECT_CUI', 'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY',\n",
    "                  'SUBJECT_TEXT', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', \n",
    "                  'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE']\n",
    "\n",
    "# Extract subject entities\n",
    "subject_entities = merged_df[subject_columns].drop_duplicates()\n",
    "\n",
    "# Rename columns to prepare for merging with object entities\n",
    "concept_columns = ['CUI', 'NAME', 'SEMTYPE', 'NOVELTY', 'TEXT', \n",
    "                 'DIST', 'MAXDIST', 'START_INDEX', 'END_INDEX', 'SCORE']\n",
    "\n",
    "subject_entities.columns = concept_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_columns = ['SUBJECT_CUI', 'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY',\n",
    "                  'SUBJECT_TEXT', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', \n",
    "                  'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE']\n",
    "\n",
    "# Extract subject entities\n",
    "subject_entities = merged_df[subject_columns].drop_duplicates()\n",
    "\n",
    "# Rename columns to prepare for merging with object entities\n",
    "concept_columns = ['CUI', 'NAME', 'SEMTYPE', 'NOVELTY', 'TEXT', \n",
    "                 'DIST', 'MAXDIST', 'START_INDEX', 'END_INDEX', 'SCORE']\n",
    "\n",
    "subject_entities.columns = concept_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract object entities using the same structure\n",
    "object_columns = ['OBJECT_CUI', 'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY',\n",
    "                 'OBJECT_TEXT', 'OBJECT_DIST', 'OBJECT_MAXDIST', \n",
    "                 'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE']\n",
    "\n",
    "object_entities = merged_df[object_columns].drop_duplicates()\n",
    "object_entities.columns = concept_columns\n",
    "\n",
    "# Combine subject and object entities and remove duplicates based on CUI\n",
    "concept_df = pd.concat([subject_entities, object_entities]).drop_duplicates(subset=['CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df.to_csv('data/concept.csv', index=False, quoting=csv.QUOTE_ALL, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract object entities using the same structure\n",
    "object_columns = ['object_cui:STRING', 'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY',\n",
    "                 'OBJECT_TEXT', 'OBJECT_DIST', 'OBJECT_MAXDIST', \n",
    "                 'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE']\n",
    "\n",
    "object_entities = merged_df[object_columns].drop_duplicates()\n",
    "object_entities.columns = concept_columns\n",
    "\n",
    "# Combine subject and object entities and remove duplicates based on CUI\n",
    "concept_df = pd.concat([subject_entities, object_entities]).drop_duplicates(subset=['CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predication dataframe shape: {predication_df.shape}\")\n",
    "print(f\"Entity dataframe shape: {concept_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df.to_csv(\"predication.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df.to_csv(\"concept.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Relationships\n",
    "The following code will create a CSV with all the connections between the concepts and predicates in a format that is easily digestible by Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df = pd.read_csv(\"data/predication.csv\", names=['PREDICATION_ID','SENTENCE_ID','PMID','PREDICATE','SUBJECT_CUI','OBJECT_CUI','INDICATOR_TYPE','PREDICATE_START_INDEX','PREDICATE_END_INDEX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for connections\n",
    "connections_columns = ['src_node', 'dest_node', 'label']\n",
    "connections_df = pd.DataFrame(columns=connections_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Connections between predication instances and subjects (inst_subject)\n",
    "inst_subject_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['PREDICATION_ID'],\n",
    "    'dest_node': predication_df['SUBJECT_CUI'],\n",
    "    'label': 'inst_subject'\n",
    "})\n",
    "\n",
    "# 2. Connections between predication instances and objects (inst_object)\n",
    "inst_object_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['PREDICATION_ID'],\n",
    "    'dest_node': predication_df['OBJECT_CUI'],\n",
    "    'label': 'inst_object'\n",
    "})\n",
    "\n",
    "# 3. Connections between subjects and objects (using PREDICATE as the label)\n",
    "subject_object_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['SUBJECT_CUI'],\n",
    "    'dest_node': predication_df['OBJECT_CUI'],\n",
    "    'label': predication_df['PREDICATE']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all connections into the final connections dataframe\n",
    "connections_df = pd.concat([\n",
    "    inst_subject_connections,\n",
    "    inst_object_connections,\n",
    "    subject_object_connections\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index for the final dataframe\n",
    "connections_df = connections_df.reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Connections dataframe shape: {connections_df.shape}\")\n",
    "print(connections_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_df.to_csv(\"connections.csv\", index=False, quoting=csv.QUOTE_ALL, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Structure\n",
    "The Neo4j import requires that you format your CSV with the datatypes in the header so here I am re-labeling the CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For entity/concept file\n",
    "# concept_df = pd.read_csv(\"data/concept.csv\", index_col=0)\n",
    "# concept_df.columns = [\n",
    "#     \"cui\", \n",
    "#     \"name:STRING\", \n",
    "#     \"semtype:LABEL\", \n",
    "#     \"novelty:FLOAT\", \n",
    "#     \"text:STRING\", \n",
    "#     \"dist\", \n",
    "#     \"maxdist\", \n",
    "#     \"start_index\", \n",
    "#     \"end_index\", \n",
    "#     \"score\", \n",
    "#     \"uuid\"\n",
    "# ]\n",
    "# concept_df.to_csv(\"entity_neo4j.csv\", index=False)\n",
    "\n",
    "# For predication file - now with predicate as LABEL\n",
    "predication_df = pd.read_csv(\"data/predication.csv\")\n",
    "predication_df.columns = [\n",
    "    \"predication_id:ID\", \n",
    "    \"sentence_id\", \n",
    "    \"pmid:STRING\", \n",
    "    \"predicate:LABEL\",  # Changed from STRING to LABEL\n",
    "    \"subject_cui:STRING\", \n",
    "    \"object_cui:STRING\", \n",
    "    \"indicator_type:STRING\", \n",
    "    \"predicate_start_index\", \n",
    "    \"predicate_end_index\"\n",
    "]\n",
    "predication_df.to_csv(\"predication_neo4j.csv\", index=False)\n",
    "\n",
    "# For connections/relationships file\n",
    "connections_df = pd.read_csv(\"data/connections.csv\")\n",
    "connections_df.columns = [\n",
    "    \":START_ID\", \n",
    "    \":END_ID\", \n",
    "    \":TYPE\"\n",
    "]\n",
    "connections_df.to_csv(\"relationships_neo4j.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.read_csv(\"concept.csv\")\n",
    "predication_df = pd.read_csv(\"predication.csv\")\n",
    "connections_df = pd.read_csv(\"connections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of nodes, relationships and properties\n",
    "print(\"\\n=== GRAPH STATISTICS ===\")\n",
    "\n",
    "# Count of nodes by type\n",
    "print(\"\\nNODE COUNTS:\")\n",
    "print(f\"Concept/Entity nodes: {len(concept_df)}\")\n",
    "print(f\"Predication nodes: {len(predication_df)}\")\n",
    "print(f\"Total nodes: {len(concept_df) + len(predication_df)}\")\n",
    "\n",
    "# Count of relationships by type\n",
    "print(\"\\nRELATIONSHIP COUNTS:\")\n",
    "relationship_counts = connections_df[':TYPE'].value_counts()\n",
    "print(\"Top 10 relationship types:\")\n",
    "print(relationship_counts.head(10))\n",
    "print(f\"Total relationships: {len(connections_df)}\")\n",
    "\n",
    "# Count of unique predicates\n",
    "print(\"\\nUNIQUE PREDICATES:\")\n",
    "unique_predicates = predication_df['predicate:LABEL'].nunique()\n",
    "print(f\"Number of unique predicates: {unique_predicates}\")\n",
    "print(\"Most common predicates:\")\n",
    "print(predication_df['predicate:LABEL'].value_counts().head(10))\n",
    "\n",
    "# Property statistics\n",
    "print(\"\\nPROPERTY STATISTICS:\")\n",
    "print(\"Entity properties:\")\n",
    "for col in concept_df.columns:\n",
    "    prop_name = col.split(':')[0]\n",
    "    non_null = concept_df[col].count()\n",
    "    print(f\"  - {prop_name}: {non_null} non-null values ({non_null/len(concept_df):.2%} coverage)\")\n",
    "\n",
    "print(\"\\nPredication properties:\")\n",
    "for col in predication_df.columns:\n",
    "    prop_name = col.split(':')[0]\n",
    "    non_null = predication_df[col].count()\n",
    "    print(f\"  - {prop_name}: {non_null} non-null values ({non_null/len(predication_df):.2%} coverage)\")\n",
    "\n",
    "# Graph density analysis\n",
    "print(\"\\nGRAPH DENSITY ANALYSIS:\")\n",
    "num_nodes = len(concept_df) + len(predication_df)\n",
    "num_edges = len(connections_df)\n",
    "max_possible_edges = num_nodes * (num_nodes - 1) / 2  # for undirected graph\n",
    "graph_density = num_edges / max_possible_edges\n",
    "print(f\"Graph density: {graph_density:.8f}\")\n",
    "\n",
    "# Distribution of connections per node\n",
    "print(\"\\nCONNECTION DISTRIBUTION:\")\n",
    "src_connections = connections_df[':START_ID'].value_counts()\n",
    "dest_connections = connections_df[':END_ID'].value_counts()\n",
    "\n",
    "print(\"Source node connection statistics:\")\n",
    "print(f\"  - Mean connections per node: {src_connections.mean():.2f}\")\n",
    "print(f\"  - Median connections per node: {src_connections.median():.2f}\")\n",
    "print(f\"  - Max connections: {src_connections.max()}\")\n",
    "\n",
    "print(\"Destination node connection statistics:\")\n",
    "print(f\"  - Mean connections per node: {dest_connections.mean():.2f}\")\n",
    "print(f\"  - Median connections per node: {dest_connections.median():.2f}\")\n",
    "print(f\"  - Max connections: {dest_connections.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_semmed_data():\n",
    "    \"\"\"\n",
    "    Validate the SemMedDB processed data for Neo4j import:\n",
    "    1. Check data types for all columns\n",
    "    2. Verify node uniqueness (no duplicates)\n",
    "    3. Verify relationship integrity (start/end nodes exist)\n",
    "    4. Check for missing values in key fields\n",
    "    \"\"\"\n",
    "    print(\"Starting SemMedDB data validation...\\n\")\n",
    "    \n",
    "    # Load the three dataframes with low_memory=False to avoid dtype warnings\n",
    "    print(\"Loading data...\")\n",
    "    concept_df = pd.read_csv(\"concept.csv\", low_memory=False)\n",
    "    predication_df = pd.read_csv(\"predication.csv\", low_memory=False)\n",
    "    connections_df = pd.read_csv(\"connections.csv\", low_memory=False)\n",
    "    \n",
    "    # ===== 1. DATA TYPES VALIDATION =====\n",
    "    print(\"\\n===== DATA TYPES VALIDATION =====\")\n",
    "    \n",
    "    # Expected data types for concept_df\n",
    "    concept_dtypes = {\n",
    "        'cui:ID': str,\n",
    "        'name:STRING': str,\n",
    "        'semtype:LABEL': str,\n",
    "        'novelty:FLOAT': float,\n",
    "        'text:STRING': str,\n",
    "        'dist:INTEGER': int,\n",
    "        'maxdist:INTEGER': int,\n",
    "        'start_index:INTEGER': int,\n",
    "        'end_index:INTEGER': int,\n",
    "        'score:INTEGER': int\n",
    "    }\n",
    "    \n",
    "    # Expected data types for predication_df\n",
    "    predication_dtypes = {\n",
    "        'predication_id:ID': int,\n",
    "        'sentence_id:INTEGER': int,\n",
    "        'pmid:STRING': str,\n",
    "        'predicate:LABEL': str,\n",
    "        'subject_cui:STRING': str,\n",
    "        'object_cui:STRING': str,\n",
    "        'indicator_type:STRING': str,\n",
    "        'predicate_start_index:INTEGER': int,\n",
    "        'predicate_end_index:INTEGER': int\n",
    "    }\n",
    "    \n",
    "    # Expected data types for connections_df\n",
    "    connections_dtypes = {\n",
    "        ':START_ID': str,\n",
    "        ':END_ID': str,\n",
    "        ':TYPE': str\n",
    "    }\n",
    "    \n",
    "    # Check data types for concept_df\n",
    "    print(\"\\nChecking concept_df data types:\")\n",
    "    for col, expected_type in concept_dtypes.items():\n",
    "        if col in concept_df.columns:\n",
    "            # Get actual type and handle mixed types\n",
    "            actual_type = concept_df[col].dtype\n",
    "            if pd.api.types.is_numeric_dtype(actual_type) and expected_type in [int, float]:\n",
    "                is_valid = True\n",
    "            elif pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                is_valid = True\n",
    "            else:\n",
    "                # For mixed types, check if conversion is possible\n",
    "                try:\n",
    "                    concept_df[col].astype(expected_type)\n",
    "                    is_valid = True\n",
    "                except:\n",
    "                    is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            if not is_valid:\n",
    "                # Show sample of problematic values\n",
    "                print(f\"    Sample values: {concept_df[col].head(3).tolist()}\")\n",
    "                \n",
    "                # Try to identify specific issues\n",
    "                if expected_type == int:\n",
    "                    non_int_mask = ~concept_df[col].astype(str).str.match(r'^-?\\d+$', na=False)\n",
    "                    print(f\"    Problematic non-integer values: {concept_df.loc[non_int_mask, col].head(3).tolist()}\")\n",
    "    \n",
    "    # Check data types for predication_df\n",
    "    print(\"\\nChecking predication_df data types:\")\n",
    "    for col, expected_type in predication_dtypes.items():\n",
    "        if col in predication_df.columns:\n",
    "            actual_type = predication_df[col].dtype\n",
    "            if pd.api.types.is_numeric_dtype(actual_type) and expected_type in [int, float]:\n",
    "                is_valid = True\n",
    "            elif pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                is_valid = True\n",
    "            else:\n",
    "                try:\n",
    "                    predication_df[col].astype(expected_type)\n",
    "                    is_valid = True\n",
    "                except:\n",
    "                    is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            if not is_valid:\n",
    "                print(f\"    Sample values: {predication_df[col].head(3).tolist()}\")\n",
    "                if expected_type == int:\n",
    "                    non_int_mask = ~predication_df[col].astype(str).str.match(r'^-?\\d+$', na=False)\n",
    "                    print(f\"    Problematic non-integer values: {predication_df.loc[non_int_mask, col].head(3).tolist()}\")\n",
    "    \n",
    "    # Check data types for connections_df\n",
    "    print(\"\\nChecking connections_df data types:\")\n",
    "    for col, expected_type in connections_dtypes.items():\n",
    "        if col in connections_df.columns:\n",
    "            actual_type = connections_df[col].dtype\n",
    "            \n",
    "            # For ID columns, check if they are strings or can be converted to strings\n",
    "            if col in [':START_ID', ':END_ID']:\n",
    "                if pd.api.types.is_string_dtype(actual_type):\n",
    "                    is_valid = True\n",
    "                else:\n",
    "                    # Check if all values can be converted to string without issues\n",
    "                    try:\n",
    "                        connections_df[col].astype(str)\n",
    "                        is_valid = True\n",
    "                        print(f\"    Note: {col} will need conversion to string type\")\n",
    "                    except:\n",
    "                        is_valid = False\n",
    "            else:\n",
    "                # For other columns\n",
    "                if pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                    is_valid = True\n",
    "                else:\n",
    "                    try:\n",
    "                        connections_df[col].astype(expected_type)\n",
    "                        is_valid = True\n",
    "                    except:\n",
    "                        is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            # Show sample of values to verify\n",
    "            print(f\"    Sample values: {connections_df[col].head(3).tolist()}\")\n",
    "            \n",
    "            # For ID columns, analyze value types\n",
    "            if col in [':START_ID', ':END_ID']:\n",
    "                num_values = connections_df[col].count()\n",
    "                num_numeric = connections_df[col].apply(lambda x: isinstance(x, (int, float)) or \n",
    "                                                      (isinstance(x, str) and x.isdigit())).sum()\n",
    "                num_string = num_values - num_numeric\n",
    "                \n",
    "                print(f\"    Value type breakdown: {num_numeric} numeric ({num_numeric/num_values:.2%}), \"\n",
    "                      f\"{num_string} non-numeric ({num_string/num_values:.2%})\")\n",
    "    \n",
    "    # ===== 2. MISSING VALUES VALIDATION =====\n",
    "    print(\"\\n===== MISSING VALUES VALIDATION =====\")\n",
    "    \n",
    "    # Check for missing values in key fields\n",
    "    print(\"\\nConcept DataFrame Missing Values in Key Fields:\")\n",
    "    missing_concept = concept_df['cui:ID'].isnull().sum()\n",
    "    print(f\"  - Primary key 'cui:ID': {missing_concept} missing values ({missing_concept/len(concept_df):.2%})\")\n",
    "    \n",
    "    print(\"\\nPredication DataFrame Missing Values in Key Fields:\")\n",
    "    missing_pred_id = predication_df['predication_id:ID'].isnull().sum()\n",
    "    missing_subject = predication_df['subject_cui:STRING'].isnull().sum()\n",
    "    missing_object = predication_df['object_cui:STRING'].isnull().sum()\n",
    "    print(f\"  - Primary key 'predication_id:ID': {missing_pred_id} missing values ({missing_pred_id/len(predication_df):.2%})\")\n",
    "    print(f\"  - Foreign key 'subject_cui:STRING': {missing_subject} missing values ({missing_subject/len(predication_df):.2%})\")\n",
    "    print(f\"  - Foreign key 'object_cui:STRING': {missing_object} missing values ({missing_object/len(predication_df):.2%})\")\n",
    "    \n",
    "    print(\"\\nConnections DataFrame Missing Values in Key Fields:\")\n",
    "    missing_src = connections_df[':START_ID'].isnull().sum()\n",
    "    missing_dest = connections_df[':END_ID'].isnull().sum()\n",
    "    missing_label = connections_df[':TYPE'].isnull().sum()\n",
    "    print(f\"  - ':START_ID': {missing_src} missing values ({missing_src/len(connections_df):.2%})\")\n",
    "    print(f\"  - ':END_ID': {missing_dest} missing values ({missing_dest/len(connections_df):.2%})\")\n",
    "    print(f\"  - ':TYPE': {missing_label} missing values ({missing_label/len(connections_df):.2%})\")\n",
    "    \n",
    "    # ===== 3. NODE UNIQUENESS VALIDATION =====\n",
    "    print(\"\\n===== NODE UNIQUENESS VALIDATION =====\")\n",
    "    \n",
    "    # Check for duplicate CUIs in concept_df\n",
    "    duplicate_cuis = concept_df['cui:ID'].duplicated().sum()\n",
    "    print(f\"Duplicate CUIs in concept_df: {duplicate_cuis} ({duplicate_cuis/len(concept_df):.2%})\")\n",
    "    \n",
    "    # Check for duplicate PREDICATION_IDs in predication_df\n",
    "    duplicate_preds = predication_df['predication_id:ID'].duplicated().sum()\n",
    "    print(f\"Duplicate PREDICATION_IDs in predication_df: {duplicate_preds} ({duplicate_preds/len(predication_df):.2%})\")\n",
    "    \n",
    "    # ===== 4. RELATIONSHIP INTEGRITY VALIDATION =====\n",
    "    print(\"\\n===== RELATIONSHIP INTEGRITY VALIDATION =====\")\n",
    "    \n",
    "    # Get distinct node IDs from the respective dataframes\n",
    "    concept_ids = set(concept_df['cui:ID'].dropna().astype(str))\n",
    "    predication_ids = set(predication_df['predication_id:ID'].dropna().astype(str))\n",
    "    \n",
    "    # All valid node IDs (combined)\n",
    "    all_valid_nodes = concept_ids.union(predication_ids)\n",
    "    \n",
    "    # Ensure IDs are strings for comparison\n",
    "    connections_df['start_id_str'] = connections_df[':START_ID'].astype(str)\n",
    "    connections_df['end_id_str'] = connections_df[':END_ID'].astype(str)\n",
    "    \n",
    "    # Check if relationship src_nodes exist\n",
    "    src_nodes = set(connections_df['start_id_str'].dropna())\n",
    "    invalid_src_nodes = src_nodes - all_valid_nodes\n",
    "    invalid_src_count = len(invalid_src_nodes)\n",
    "    \n",
    "    print(f\"\\nInvalid source nodes in relationships: {invalid_src_count} ({invalid_src_count/len(src_nodes):.2%})\")\n",
    "    if invalid_src_count > 0 and invalid_src_count <= 10:\n",
    "        print(f\"  Sample of invalid source nodes: {list(invalid_src_nodes)[:10]}\")\n",
    "    \n",
    "    # Check if relationship dest_nodes exist\n",
    "    dest_nodes = set(connections_df['end_id_str'].dropna())\n",
    "    invalid_dest_nodes = dest_nodes - all_valid_nodes\n",
    "    invalid_dest_count = len(invalid_dest_nodes)\n",
    "    \n",
    "    print(f\"\\nInvalid destination nodes in relationships: {invalid_dest_count} ({invalid_dest_count/len(dest_nodes):.2%})\")\n",
    "    if invalid_dest_count > 0 and invalid_dest_count <= 10:\n",
    "        print(f\"  Sample of invalid destination nodes: {list(invalid_dest_nodes)[:10]}\")\n",
    "    \n",
    "    # ===== 5. SUMMARY =====\n",
    "    print(\"\\n===== VALIDATION SUMMARY =====\")\n",
    "    \n",
    "    # Count total issues\n",
    "    total_issues = (missing_concept + missing_pred_id + missing_subject + missing_object + \n",
    "                    missing_src + missing_dest + missing_label +\n",
    "                    duplicate_cuis + duplicate_preds +\n",
    "                    invalid_src_count + invalid_dest_count)\n",
    "    \n",
    "    if total_issues == 0:\n",
    "        print(\"✅ All validations passed! Data appears clean and ready for Neo4j import.\")\n",
    "    else:\n",
    "        print(f\"❌ Found {total_issues} total issues that may affect your Neo4j import.\")\n",
    "        \n",
    "        # Recommend fixes based on issues found\n",
    "        print(\"\\nRecommended fixes:\")\n",
    "        \n",
    "        if missing_concept + missing_pred_id > 0:\n",
    "            print(\"  - Remove rows with missing primary keys (cui:ID or predication_id:ID)\")\n",
    "        \n",
    "        if missing_subject + missing_object > 0:\n",
    "            print(\"  - Fix or remove predications with missing subject_cui:STRING or object_cui:STRING\")\n",
    "        \n",
    "        if missing_src + missing_dest + missing_label > 0:\n",
    "            print(\"  - Remove relationships with missing :START_ID, :END_ID, or :TYPE\")\n",
    "        \n",
    "        if duplicate_cuis > 0:\n",
    "            print(\"  - Remove duplicate CUIs or merge their properties\")\n",
    "        \n",
    "        if duplicate_preds > 0:\n",
    "            print(\"  - Remove duplicate predication_id:IDs\")\n",
    "        \n",
    "        if invalid_src_count + invalid_dest_count > 0:\n",
    "            print(\"  - Remove relationships with invalid source or destination nodes\")\n",
    "            \n",
    "        # Check if START_ID and END_ID need type conversion\n",
    "        if not pd.api.types.is_string_dtype(connections_df[':START_ID'].dtype) or \\\n",
    "           not pd.api.types.is_string_dtype(connections_df[':END_ID'].dtype):\n",
    "            print(\"  - Convert ':START_ID' and ':END_ID' columns to string type (see fix_connections_dtypes function)\")\n",
    "\n",
    "def fix_connections_dtypes(file_path=\"connections.csv\"):\n",
    "    \"\"\"\n",
    "    Fix data types in the connections dataframe:\n",
    "    - Convert :START_ID and :END_ID to string type\n",
    "    - Save the fixed version\n",
    "    \"\"\"\n",
    "    # Load the connections CSV\n",
    "    connections_df = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "    # Convert src_node and dest_node to string\n",
    "    connections_df[':START_ID'] = connections_df[':START_ID'].astype(str)\n",
    "    connections_df[':END_ID'] = connections_df[':END_ID'].astype(str)\n",
    "    \n",
    "    # Save the fixed version\n",
    "    connections_df.to_csv(\"connections_fixed.csv\", index=False)\n",
    "    print(f\"Fixed connections data saved to 'connections_fixed.csv'\")\n",
    "    return connections_df\n",
    "\n",
    "validate_semmed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the :ID col to uuid instead of cui since some of them are compound cuis\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# Function to generate UUID\n",
    "def generate_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# Read the files\n",
    "concept_df = pd.read_csv(\"data/concept.csv\")\n",
    "connections_df = pd.read_csv(\"data/connections.csv\")\n",
    "\n",
    "# Add UUID column to concept_df and rename CUI column\n",
    "concept_df[':ID'] = [generate_uuid() for _ in range(len(concept_df))]\n",
    "concept_df.rename(columns={'cui:ID': 'cui:STRING'}, inplace=True)\n",
    "\n",
    "# Create a mapping dictionary from CUI to UUID\n",
    "cui_to_uuid = dict(zip(concept_df['cui:STRING'], concept_df[':ID']))\n",
    "\n",
    "# Update connections in connections_df where END_ID matches a CUI pattern (including compound CUIs)\n",
    "cui_mask = connections_df[':END_ID'].str.contains(r'C\\d+', regex=True, na=False)\n",
    "connections_df.loc[cui_mask, ':END_ID'] = connections_df.loc[cui_mask, ':END_ID'].map(cui_to_uuid)\n",
    "\n",
    "# Save the modified files\n",
    "concept_df.to_csv(\"concept_updated.csv\", index=False)\n",
    "connections_df.to_csv(\"connections_updated.csv\", index=False)\n",
    "\n",
    "# Display sample of the changes\n",
    "print(\"\\nUpdated concept.csv sample:\")\n",
    "print(concept_df.head(3).to_string())\n",
    "print(\"\\nUpdated connections.csv sample:\")\n",
    "print(connections_df.head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for predication rows where subject_cui or object_cui doesn't match the C******* pattern\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the predication data\n",
    "predication_df = pd.read_csv(\"data/predication.csv\")\n",
    "\n",
    "# Define a function to check if a CUI follows the standard format\n",
    "def is_not_standard_cui(cui_str):\n",
    "    # Check if the string is None or NaN\n",
    "    if pd.isna(cui_str):\n",
    "        return True\n",
    "    \n",
    "    # Check if the string doesn't match the C******* pattern\n",
    "    # This will also catch compound CUIs with pipe separators\n",
    "    if not re.match(r'^C\\d+$', str(cui_str)):\n",
    "        # If it's a compound CUI with pipe separator, check each part\n",
    "        if '|' in str(cui_str):\n",
    "            parts = str(cui_str).split('|')\n",
    "            # If any part doesn't match C******* or is purely numeric, flag it\n",
    "            return any(not (re.match(r'^C\\d+$', part) or part.isdigit()) for part in parts)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Find rows where subject_cui doesn't match the pattern\n",
    "non_standard_subject = predication_df[predication_df['subject_cui:STRING'].apply(is_not_standard_cui)]\n",
    "\n",
    "# Find rows where object_cui doesn't match the pattern\n",
    "non_standard_object = predication_df[predication_df['object_cui:STRING'].apply(is_not_standard_cui)]\n",
    "\n",
    "# Display the results\n",
    "print(\"Rows where subject_cui doesn't match the C******* pattern:\")\n",
    "if len(non_standard_subject) > 0:\n",
    "    print(non_standard_subject)\n",
    "else:\n",
    "    print(\"No rows found with non-standard subject_cui format.\")\n",
    "\n",
    "print(\"\\nRows where object_cui doesn't match the C******* pattern:\")\n",
    "if len(non_standard_object) > 0:\n",
    "    print(non_standard_object)\n",
    "else:\n",
    "    print(\"No rows found with non-standard object_cui format.\")\n",
    "\n",
    "# Count of non-standard CUIs\n",
    "print(f\"\\nTotal rows with non-standard subject_cui: {len(non_standard_subject)}\")\n",
    "print(f\"Total rows with non-standard object_cui: {len(non_standard_object)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the predication data\n",
    "df = pd.read_csv(\"semmed_data/predication.csv\", \n",
    "                 names=predication_headers,\n",
    "                 encoding='ISO-8859-1',  # Add this parameter\n",
    "                 on_bad_lines='warn',    # Optionally handle bad lines\n",
    "                 na_values=['\\\\N'])      # Handle NULL values\n",
    "\n",
    "def is_non_standard_cui(cui):\n",
    "    # Handle NaN/None values\n",
    "    if pd.isna(cui):\n",
    "        return True\n",
    "        \n",
    "    cui_str = str(cui)\n",
    "    # Split for compound CUIs\n",
    "    cuis = cui_str.split('|')\n",
    "    \n",
    "    for single_cui in cuis:\n",
    "        # Check if it matches the standard C******* pattern\n",
    "        if not re.match(r'^C\\d+$', single_cui):\n",
    "            # If it's a pure number (like \"3075\" in compound CUIs), it's acceptable\n",
    "            if not single_cui.isdigit():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Find non-standard CUIs\n",
    "non_standard_subjects = df[df['SUBJECT_CUI'].apply(is_non_standard_cui)]\n",
    "non_standard_objects = df[df['OBJECT_CUI'].apply(is_non_standard_cui)]\n",
    "\n",
    "# Print summary\n",
    "print(\"=== Non-standard CUI Analysis ===\")\n",
    "print(f\"\\nTotal rows in dataset: {len(df)}\")\n",
    "print(f\"Rows with non-standard subject CUIs: {len(non_standard_subjects)}\")\n",
    "print(f\"Rows with non-standard object CUIs: {len(non_standard_objects)}\")\n",
    "\n",
    "# Show sample of non-standard entries\n",
    "print(\"\\nSample of non-standard subject CUIs:\")\n",
    "print(non_standard_subjects[['PREDICATION_ID', 'SUBJECT_CUI', 'SUBJECT_NAME']].head())\n",
    "\n",
    "print(\"\\nSample of non-standard object CUIs:\")\n",
    "print(non_standard_objects[['PREDICATION_ID', 'OBJECT_CUI', 'OBJECT_NAME']].head())\n",
    "\n",
    "# Get unique patterns of non-standard CUIs\n",
    "print(\"\\nUnique patterns of non-standard subject CUIs:\")\n",
    "print(non_standard_subjects['SUBJECT_CUI'].unique()[:10])\n",
    "\n",
    "print(\"\\nUnique patterns of non-standard object CUIs:\")\n",
    "print(non_standard_objects['OBJECT_CUI'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Output all non-standard CUIs to a CSV file\n",
    "output_file = \"non_standard_cuis.csv\"\n",
    "\n",
    "# Get all unique non-standard CUIs\n",
    "subject_cuis = non_standard_subjects['SUBJECT_CUI'].dropna().unique().tolist()\n",
    "object_cuis = non_standard_objects['OBJECT_CUI'].dropna().unique().tolist()\n",
    "\n",
    "# Combine all unique non-standard CUIs\n",
    "all_non_standard_cuis = list(set(subject_cuis + object_cuis))\n",
    "\n",
    "# Write to CSV file (one CUI per line)\n",
    "with open(output_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['cui'])  # Header\n",
    "    for cui in all_non_standard_cuis:\n",
    "        writer.writerow([cui])\n",
    "\n",
    "print(f\"All {len(all_non_standard_cuis)} unique non-standard CUIs have been written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a specific CUI in the predication.csv file\n",
    "import pandas as pd\n",
    "\n",
    "# Define the CUI to search for\n",
    "search_cui = \"7523\"\n",
    "\n",
    "# Read the predication.csv file\n",
    "# Note: Adjust the file path if needed\n",
    "# First try to read from the original predication.csv file\n",
    "pred_df = pd.read_csv(\"semmed_data/predication.csv\", \n",
    "                names=predication_headers,\n",
    "                encoding='ISO-8859-1',  # Add this parameter\n",
    "                on_bad_lines='warn',    # Optionally handle bad lines\n",
    "                na_values=['\\\\N']) \n",
    "\n",
    "# Search for the CUI in either subject or object columns\n",
    "matching_rows = pred_df[(pred_df['SUBJECT_CUI'].astype(str).str.contains(search_cui)) | \n",
    "                        (pred_df['OBJECT_CUI'].astype(str).str.contains(search_cui))]\n",
    "\n",
    "if len(matching_rows) > 0:\n",
    "    print(f\"Found {len(matching_rows)} rows containing CUI '{search_cui}':\")\n",
    "    print(matching_rows)\n",
    "else:\n",
    "    print(f\"No rows found containing CUI '{search_cui}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_predication_csv():\n",
    "    input_path = 'data/predication.csv'\n",
    "    output_path = 'data/predication_prefixed.csv'\n",
    "    \n",
    "    # Read the predication header to get column names\n",
    "    with open('data/predication_header.csv', 'r') as header_file:\n",
    "        header_reader = csv.reader(header_file)\n",
    "        headers = next(header_reader)\n",
    "    \n",
    "    # Process the predication file\n",
    "    print(\"Processing predication.csv to add K prefix to non-C CUIs...\")\n",
    "    \n",
    "    # Read the file in chunks to handle large files efficiently\n",
    "    chunk_size = 100000\n",
    "    chunks_processed = 0\n",
    "    examples_shown = 0\n",
    "    \n",
    "    with open(output_path, 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(headers)  # Write header row\n",
    "        \n",
    "        for chunk in pd.read_csv(input_path, names=headers, chunksize=chunk_size):\n",
    "            modified_rows = 0\n",
    "            \n",
    "            for _, row in chunk.iterrows():\n",
    "                row_data = row.tolist()\n",
    "                \n",
    "                # Check and modify SUBJECT_CUI if it doesn't start with 'C'\n",
    "                if isinstance(row_data[4], str) and not row_data[4].startswith('C'):\n",
    "                    row_data[4] = f'K{row_data[4]}'\n",
    "                    modified_rows += 1\n",
    "                \n",
    "                # Check and modify OBJECT_CUI if it doesn't start with 'C'\n",
    "                if isinstance(row_data[5], str) and not row_data[5].startswith('C'):\n",
    "                    row_data[5] = f'K{row_data[5]}'\n",
    "                    modified_rows += 1\n",
    "                \n",
    "                writer.writerow(row_data)\n",
    "                \n",
    "                # Print a few examples of modified rows\n",
    "                if modified_rows > 0 and examples_shown < 5 and (row_data[4].startswith('K') or row_data[5].startswith('K')):\n",
    "                    print(f\"Modified row: {row_data}\")\n",
    "                    examples_shown += 1\n",
    "            \n",
    "            chunks_processed += 1\n",
    "            print(f\"Processed chunk {chunks_processed}, modified {modified_rows} CUIs\")\n",
    "    \n",
    "    print(f\"Processing complete. Check {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_predication_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.read_csv(\"data/concept_prefixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df.to_csv(\"concept.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to add a column\n",
    "import csv\n",
    "\n",
    "def add_concept_column(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r', newline='') as infile, \\\n",
    "             open(output_file, 'w', newline='') as outfile:\n",
    "            \n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "            \n",
    "            # Process each row and add 'Concept'\n",
    "            for row in reader:\n",
    "                row.append('Predication')\n",
    "                writer.writerow(row)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Usage\n",
    "add_concept_column('data/predication.csv', 'predication.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility script to restore quotes on a CSV\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/concept.csv'); df.to_csv('output.csv', quoting=1, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
