{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column headers\n",
    "predication_headers = [\n",
    "    'PREDICATION_ID', 'SENTENCE_ID', 'PMID', 'PREDICATE', 'SUBJECT_CUI',\n",
    "    'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY', 'OBJECT_CUI',\n",
    "    'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY', 'FACT_VALUE_CHAR',\n",
    "    'MOD_SCALE_CHAR', 'MOD_VALUE_FLOAT'\n",
    "]\n",
    "\n",
    "predication_aux_headers = [\n",
    "    'PREDICATION_AUX_ID', 'PREDICATION_ID', 'SUBJECT_TEXT', 'SUBJECT_DIST',\n",
    "    'SUBJECT_MAXDIST', 'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE',\n",
    "    'INDICATOR_TYPE', 'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX', 'OBJECT_TEXT',\n",
    "    'OBJECT_DIST', 'OBJECT_MAXDIST', 'OBJECT_START_INDEX', 'OBJECT_END_INDEX',\n",
    "    'OBJECT_SCORE', 'CURR_TIMESTAMP'\n",
    "]\n",
    "\n",
    "# predication_dtype = {\n",
    "#     'PREDICATION_ID': 'int32',\n",
    "#     'SENTENCE_ID': 'int32',\n",
    "#     'PMID': 'str',\n",
    "#     'PREDICATE': 'str',\n",
    "#     'SUBJECT_CUI': 'str',\n",
    "#     'SUBJECT_NAME': 'str',\n",
    "#     'SUBJECT_SEMTYPE': 'str',\n",
    "#     'SUBJECT_NOVELTY': 'int8',\n",
    "#     'object_cui:STRING': 'str',\n",
    "#     'OBJECT_NAME': 'str',\n",
    "#     'OBJECT_SEMTYPE': 'str',\n",
    "#     'OBJECT_NOVELTY': 'int8',\n",
    "#     'FACT_VALUE_CHAR': 'str',\n",
    "#     'MOD_SCALE_CHAR': 'str',\n",
    "#     'MOD_VALUE_FLOAT': 'float32'\n",
    "# }\n",
    "\n",
    "# predication_aux_dtype = {\n",
    "#     'PREDICATION_AUX_ID': 'int32',\n",
    "#     'PREDICATION_ID': 'int32',\n",
    "#     'SUBJECT_TEXT': 'str',\n",
    "#     'SUBJECT_DIST': 'int32',\n",
    "#     'SUBJECT_MAXDIST': 'int32',\n",
    "#     'SUBJECT_START_INDEX': 'int32',\n",
    "#     'SUBJECT_END_INDEX': 'int32',\n",
    "#     'SUBJECT_SCORE': 'int32',\n",
    "#     'INDICATOR_TYPE': 'str',\n",
    "#     'PREDICATE_START_INDEX': 'int32',\n",
    "#     'PREDICATE_END_INDEX': 'int32',\n",
    "#     'OBJECT_TEXT': 'str',\n",
    "#     'OBJECT_DIST': 'int32',\n",
    "#     'OBJECT_MAXDIST': 'int32',\n",
    "#     'OBJECT_START_INDEX': 'int32',\n",
    "#     'OBJECT_END_INDEX': 'int32',\n",
    "#     'OBJECT_SCORE': 'int32',\n",
    "#     'CURR_TIMESTAMP': 'string'  # Assuming timestamp is read as string\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Nodes\n",
    "The following code will create two CSVs for entity and predication nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file with the specified headers using Dask\n",
    "df = pd.read_csv('semmed_data/predication.csv', names=predication_headers, encoding='ISO-8859-1', on_bad_lines='warn', na_values=['\\\\N'])\n",
    "\n",
    "# Read the CSV file with the specified headers using Dask\n",
    "df_aux = pd.read_csv('semmed_data/predication_aux.csv', names=predication_aux_headers, encoding='ISO-8859-1', on_bad_lines='warn', na_values=['\\\\N']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export both DataFrames to a single .pkl file\n",
    "# dataframes = {'df': df, 'df_aux': df_aux}\n",
    "# dd.to_pickle(dataframes, 'semmed_data/dataframes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge with Dask\n",
    "# merged_df = dd.merge(df, df_aux, on='PREDICATION_ID', how='inner', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, df_aux, on='PREDICATION_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_columns = ['PREDICATION_ID', 'SENTENCE_ID', 'PMID', 'PREDICATE',\n",
    "                      'SUBJECT_CUI', 'OBJECT_CUI', 'INDICATOR_TYPE', \n",
    "                      'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX']\n",
    "\n",
    "predication_df = merged_df[predication_columns].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_columns = ['SUBJECT_CUI', 'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY',\n",
    "                  'SUBJECT_TEXT', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', \n",
    "                  'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE']\n",
    "\n",
    "# Extract subject entities\n",
    "subject_entities = merged_df[subject_columns].drop_duplicates()\n",
    "\n",
    "# Rename columns to prepare for merging with object entities\n",
    "concept_columns = ['CUI', 'NAME', 'SEMTYPE', 'NOVELTY', 'TEXT', \n",
    "                 'DIST', 'MAXDIST', 'START_INDEX', 'END_INDEX', 'SCORE']\n",
    "\n",
    "subject_entities.columns = concept_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract object entities using the same structure\n",
    "object_columns = ['object_cui:STRING', 'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY',\n",
    "                 'OBJECT_TEXT', 'OBJECT_DIST', 'OBJECT_MAXDIST', \n",
    "                 'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE']\n",
    "\n",
    "object_entities = merged_df[object_columns].drop_duplicates()\n",
    "object_entities.columns = concept_columns\n",
    "\n",
    "# Combine subject and object entities and remove duplicates based on CUI\n",
    "concept_df = pd.concat([subject_entities, object_entities]).drop_duplicates(subset=['CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predication dataframe shape: {predication_df.shape}\")\n",
    "print(f\"Entity dataframe shape: {concept_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df.to_csv(\"predication.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df.to_csv(\"concept.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Relationships\n",
    "The following code will create a CSV with all the connections between the concepts and predicates in a format that is easily digestible by Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df = pd.read_csv(\"predication.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for connections\n",
    "connections_columns = ['src_node', 'dest_node', 'label']\n",
    "connections_df = pd.DataFrame(columns=connections_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Connections between predication instances and subjects (inst_subject)\n",
    "inst_subject_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['PREDICATION_ID'],\n",
    "    'dest_node': predication_df['SUBJECT_CUI'],\n",
    "    'label': 'inst_subject'\n",
    "})\n",
    "\n",
    "# 2. Connections between predication instances and objects (inst_object)\n",
    "inst_object_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['PREDICATION_ID'],\n",
    "    'dest_node': predication_df['OBJECT_CUI'],\n",
    "    'label': 'inst_object'\n",
    "})\n",
    "\n",
    "# 3. Connections between subjects and objects (using PREDICATE as the label)\n",
    "subject_object_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['SUBJECT_CUI'],\n",
    "    'dest_node': predication_df['OBJECT_CUI'],\n",
    "    'label': predication_df['PREDICATE']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all connections into the final connections dataframe\n",
    "connections_df = pd.concat([\n",
    "    inst_subject_connections,\n",
    "    inst_object_connections,\n",
    "    subject_object_connections\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index for the final dataframe\n",
    "connections_df = connections_df.reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Connections dataframe shape: {connections_df.shape}\")\n",
    "print(connections_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_df.to_csv(\"connections.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Structure\n",
    "The Neo4j import requires that you format your CSV with the datatypes in the header so here I am re-labeling the CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For entity/concept file\n",
    "concept_df = pd.read_csv(\"concept.csv\", index_col=0)\n",
    "concept_df.columns = [\n",
    "    \"cui:ID\", \n",
    "    \"name:STRING\", \n",
    "    \"semtype:LABEL\", \n",
    "    \"novelty:FLOAT\", \n",
    "    \"text:STRING\", \n",
    "    \"dist:INTEGER\", \n",
    "    \"maxdist:INTEGER\", \n",
    "    \"start_index:INTEGER\", \n",
    "    \"end_index:INTEGER\", \n",
    "    \"score:INTEGER\"\n",
    "]\n",
    "concept_df.to_csv(\"entity_neo4j.csv\", index=False)\n",
    "\n",
    "# For predication file - now with predicate as LABEL\n",
    "predication_df = pd.read_csv(\"predication.csv\", index_col=0)\n",
    "predication_df.columns = [\n",
    "    \"predication_id:ID\", \n",
    "    \"sentence_id:INTEGER\", \n",
    "    \"pmid:STRING\", \n",
    "    \"predicate:LABEL\",  # Changed from STRING to LABEL\n",
    "    \"subject_cui:STRING\", \n",
    "    \"object_cui:STRING\", \n",
    "    \"indicator_type:STRING\", \n",
    "    \"predicate_start_index:INTEGER\", \n",
    "    \"predicate_end_index:INTEGER\"\n",
    "]\n",
    "predication_df.to_csv(\"predication_neo4j.csv\", index=False)\n",
    "\n",
    "# For connections/relationships file\n",
    "connections_df = pd.read_csv(\"connections.csv\", index_col=0)\n",
    "connections_df.columns = [\n",
    "    \":START_ID\", \n",
    "    \":END_ID\", \n",
    "    \":TYPE\"\n",
    "]\n",
    "connections_df.to_csv(\"relationships_neo4j.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.read_csv(\"concept.csv\")\n",
    "predication_df = pd.read_csv(\"predication.csv\")\n",
    "connections_df = pd.read_csv(\"connections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of nodes, relationships and properties\n",
    "print(\"\\n=== GRAPH STATISTICS ===\")\n",
    "\n",
    "# Count of nodes by type\n",
    "print(\"\\nNODE COUNTS:\")\n",
    "print(f\"Concept/Entity nodes: {len(concept_df)}\")\n",
    "print(f\"Predication nodes: {len(predication_df)}\")\n",
    "print(f\"Total nodes: {len(concept_df) + len(predication_df)}\")\n",
    "\n",
    "# Count of relationships by type\n",
    "print(\"\\nRELATIONSHIP COUNTS:\")\n",
    "relationship_counts = connections_df[':TYPE'].value_counts()\n",
    "print(\"Top 10 relationship types:\")\n",
    "print(relationship_counts.head(10))\n",
    "print(f\"Total relationships: {len(connections_df)}\")\n",
    "\n",
    "# Count of unique predicates\n",
    "print(\"\\nUNIQUE PREDICATES:\")\n",
    "unique_predicates = predication_df['predicate:LABEL'].nunique()\n",
    "print(f\"Number of unique predicates: {unique_predicates}\")\n",
    "print(\"Most common predicates:\")\n",
    "print(predication_df['predicate:LABEL'].value_counts().head(10))\n",
    "\n",
    "# Property statistics\n",
    "print(\"\\nPROPERTY STATISTICS:\")\n",
    "print(\"Entity properties:\")\n",
    "for col in concept_df.columns:\n",
    "    prop_name = col.split(':')[0]\n",
    "    non_null = concept_df[col].count()\n",
    "    print(f\"  - {prop_name}: {non_null} non-null values ({non_null/len(concept_df):.2%} coverage)\")\n",
    "\n",
    "print(\"\\nPredication properties:\")\n",
    "for col in predication_df.columns:\n",
    "    prop_name = col.split(':')[0]\n",
    "    non_null = predication_df[col].count()\n",
    "    print(f\"  - {prop_name}: {non_null} non-null values ({non_null/len(predication_df):.2%} coverage)\")\n",
    "\n",
    "# Graph density analysis\n",
    "print(\"\\nGRAPH DENSITY ANALYSIS:\")\n",
    "num_nodes = len(concept_df) + len(predication_df)\n",
    "num_edges = len(connections_df)\n",
    "max_possible_edges = num_nodes * (num_nodes - 1) / 2  # for undirected graph\n",
    "graph_density = num_edges / max_possible_edges\n",
    "print(f\"Graph density: {graph_density:.8f}\")\n",
    "\n",
    "# Distribution of connections per node\n",
    "print(\"\\nCONNECTION DISTRIBUTION:\")\n",
    "src_connections = connections_df[':START_ID'].value_counts()\n",
    "dest_connections = connections_df[':END_ID'].value_counts()\n",
    "\n",
    "print(\"Source node connection statistics:\")\n",
    "print(f\"  - Mean connections per node: {src_connections.mean():.2f}\")\n",
    "print(f\"  - Median connections per node: {src_connections.median():.2f}\")\n",
    "print(f\"  - Max connections: {src_connections.max()}\")\n",
    "\n",
    "print(\"Destination node connection statistics:\")\n",
    "print(f\"  - Mean connections per node: {dest_connections.mean():.2f}\")\n",
    "print(f\"  - Median connections per node: {dest_connections.median():.2f}\")\n",
    "print(f\"  - Max connections: {dest_connections.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SemMedDB data validation...\n",
      "\n",
      "Loading data...\n",
      "\n",
      "===== DATA TYPES VALIDATION =====\n",
      "\n",
      "Checking concept_df data types:\n",
      "  - cui:ID: Expected str, Got object - ✓\n",
      "  - name:STRING: Expected str, Got object - ✓\n",
      "  - semtype:LABEL: Expected str, Got object - ✓\n",
      "  - novelty:FLOAT: Expected float, Got float64 - ✓\n",
      "  - text:STRING: Expected str, Got object - ✓\n",
      "  - dist:INTEGER: Expected int, Got int64 - ✓\n",
      "  - maxdist:INTEGER: Expected int, Got int64 - ✓\n",
      "  - start_index:INTEGER: Expected int, Got int64 - ✓\n",
      "  - end_index:INTEGER: Expected int, Got int64 - ✓\n",
      "  - score:INTEGER: Expected int, Got int64 - ✓\n",
      "\n",
      "Checking predication_df data types:\n",
      "  - predication_id:ID: Expected int, Got int64 - ✓\n",
      "  - sentence_id:INTEGER: Expected int, Got int64 - ✓\n",
      "  - pmid:STRING: Expected str, Got int64 - ✓\n",
      "  - predicate:LABEL: Expected str, Got object - ✓\n",
      "  - subject_cui:STRING: Expected str, Got object - ✓\n",
      "  - object_cui:STRING: Expected str, Got object - ✓\n",
      "  - indicator_type:STRING: Expected str, Got object - ✓\n",
      "  - predicate_start_index:INTEGER: Expected int, Got int64 - ✓\n",
      "  - predicate_end_index:INTEGER: Expected int, Got object - ✗\n",
      "    Sample values: ['83', '279', '225']\n",
      "    Problematic non-integer values: ['tetrahydrofuran', 'arteriosclerosis obliterans', 'steady state']\n",
      "\n",
      "Checking connections_df data types:\n",
      "  - :START_ID: Expected str, Got object - ✓\n",
      "    Sample values: ['10592604', '10592697', '10592728']\n",
      "    Value type breakdown: 262681047 numeric (67.11%), 128759472 non-numeric (32.89%)\n",
      "  - :END_ID: Expected str, Got object - ✓\n",
      "    Sample values: ['C0003725', 'C0039258', 'C0318627']\n",
      "    Value type breakdown: 4968687 numeric (1.27%), 386471832 non-numeric (98.73%)\n",
      "  - :TYPE: Expected str, Got object - ✓\n",
      "    Sample values: ['inst_subject', 'inst_subject', 'inst_subject']\n",
      "\n",
      "===== MISSING VALUES VALIDATION =====\n",
      "\n",
      "Concept DataFrame Missing Values in Key Fields:\n",
      "  - Primary key 'cui:ID': 0 missing values (0.00%)\n",
      "\n",
      "Predication DataFrame Missing Values in Key Fields:\n",
      "  - Primary key 'predication_id:ID': 0 missing values (0.00%)\n",
      "  - Foreign key 'subject_cui:STRING': 0 missing values (0.00%)\n",
      "  - Foreign key 'object_cui:STRING': 0 missing values (0.00%)\n",
      "\n",
      "Connections DataFrame Missing Values in Key Fields:\n",
      "  - ':START_ID': 0 missing values (0.00%)\n",
      "  - ':END_ID': 0 missing values (0.00%)\n",
      "  - ':TYPE': 0 missing values (0.00%)\n",
      "\n",
      "===== NODE UNIQUENESS VALIDATION =====\n",
      "Duplicate CUIs in concept_df: 0 (0.00%)\n",
      "Duplicate PREDICATION_IDs in predication_df: 0 (0.00%)\n",
      "\n",
      "===== RELATIONSHIP INTEGRITY VALIDATION =====\n",
      "\n",
      "Invalid source nodes in relationships: 0 (0.00%)\n",
      "\n",
      "Invalid destination nodes in relationships: 0 (0.00%)\n",
      "\n",
      "===== VALIDATION SUMMARY =====\n",
      "✅ All validations passed! Data appears clean and ready for Neo4j import.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_semmed_data():\n",
    "    \"\"\"\n",
    "    Validate the SemMedDB processed data for Neo4j import:\n",
    "    1. Check data types for all columns\n",
    "    2. Verify node uniqueness (no duplicates)\n",
    "    3. Verify relationship integrity (start/end nodes exist)\n",
    "    4. Check for missing values in key fields\n",
    "    \"\"\"\n",
    "    print(\"Starting SemMedDB data validation...\\n\")\n",
    "    \n",
    "    # Load the three dataframes with low_memory=False to avoid dtype warnings\n",
    "    print(\"Loading data...\")\n",
    "    concept_df = pd.read_csv(\"concept.csv\", low_memory=False)\n",
    "    predication_df = pd.read_csv(\"predication.csv\", low_memory=False)\n",
    "    connections_df = pd.read_csv(\"connections.csv\", low_memory=False)\n",
    "    \n",
    "    # ===== 1. DATA TYPES VALIDATION =====\n",
    "    print(\"\\n===== DATA TYPES VALIDATION =====\")\n",
    "    \n",
    "    # Expected data types for concept_df\n",
    "    concept_dtypes = {\n",
    "        'cui:ID': str,\n",
    "        'name:STRING': str,\n",
    "        'semtype:LABEL': str,\n",
    "        'novelty:FLOAT': float,\n",
    "        'text:STRING': str,\n",
    "        'dist:INTEGER': int,\n",
    "        'maxdist:INTEGER': int,\n",
    "        'start_index:INTEGER': int,\n",
    "        'end_index:INTEGER': int,\n",
    "        'score:INTEGER': int\n",
    "    }\n",
    "    \n",
    "    # Expected data types for predication_df\n",
    "    predication_dtypes = {\n",
    "        'predication_id:ID': int,\n",
    "        'sentence_id:INTEGER': int,\n",
    "        'pmid:STRING': str,\n",
    "        'predicate:LABEL': str,\n",
    "        'subject_cui:STRING': str,\n",
    "        'object_cui:STRING': str,\n",
    "        'indicator_type:STRING': str,\n",
    "        'predicate_start_index:INTEGER': int,\n",
    "        'predicate_end_index:INTEGER': int\n",
    "    }\n",
    "    \n",
    "    # Expected data types for connections_df\n",
    "    connections_dtypes = {\n",
    "        ':START_ID': str,\n",
    "        ':END_ID': str,\n",
    "        ':TYPE': str\n",
    "    }\n",
    "    \n",
    "    # Check data types for concept_df\n",
    "    print(\"\\nChecking concept_df data types:\")\n",
    "    for col, expected_type in concept_dtypes.items():\n",
    "        if col in concept_df.columns:\n",
    "            # Get actual type and handle mixed types\n",
    "            actual_type = concept_df[col].dtype\n",
    "            if pd.api.types.is_numeric_dtype(actual_type) and expected_type in [int, float]:\n",
    "                is_valid = True\n",
    "            elif pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                is_valid = True\n",
    "            else:\n",
    "                # For mixed types, check if conversion is possible\n",
    "                try:\n",
    "                    concept_df[col].astype(expected_type)\n",
    "                    is_valid = True\n",
    "                except:\n",
    "                    is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            if not is_valid:\n",
    "                # Show sample of problematic values\n",
    "                print(f\"    Sample values: {concept_df[col].head(3).tolist()}\")\n",
    "                \n",
    "                # Try to identify specific issues\n",
    "                if expected_type == int:\n",
    "                    non_int_mask = ~concept_df[col].astype(str).str.match(r'^-?\\d+$', na=False)\n",
    "                    print(f\"    Problematic non-integer values: {concept_df.loc[non_int_mask, col].head(3).tolist()}\")\n",
    "    \n",
    "    # Check data types for predication_df\n",
    "    print(\"\\nChecking predication_df data types:\")\n",
    "    for col, expected_type in predication_dtypes.items():\n",
    "        if col in predication_df.columns:\n",
    "            actual_type = predication_df[col].dtype\n",
    "            if pd.api.types.is_numeric_dtype(actual_type) and expected_type in [int, float]:\n",
    "                is_valid = True\n",
    "            elif pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                is_valid = True\n",
    "            else:\n",
    "                try:\n",
    "                    predication_df[col].astype(expected_type)\n",
    "                    is_valid = True\n",
    "                except:\n",
    "                    is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            if not is_valid:\n",
    "                print(f\"    Sample values: {predication_df[col].head(3).tolist()}\")\n",
    "                if expected_type == int:\n",
    "                    non_int_mask = ~predication_df[col].astype(str).str.match(r'^-?\\d+$', na=False)\n",
    "                    print(f\"    Problematic non-integer values: {predication_df.loc[non_int_mask, col].head(3).tolist()}\")\n",
    "    \n",
    "    # Check data types for connections_df\n",
    "    print(\"\\nChecking connections_df data types:\")\n",
    "    for col, expected_type in connections_dtypes.items():\n",
    "        if col in connections_df.columns:\n",
    "            actual_type = connections_df[col].dtype\n",
    "            \n",
    "            # For ID columns, check if they are strings or can be converted to strings\n",
    "            if col in [':START_ID', ':END_ID']:\n",
    "                if pd.api.types.is_string_dtype(actual_type):\n",
    "                    is_valid = True\n",
    "                else:\n",
    "                    # Check if all values can be converted to string without issues\n",
    "                    try:\n",
    "                        connections_df[col].astype(str)\n",
    "                        is_valid = True\n",
    "                        print(f\"    Note: {col} will need conversion to string type\")\n",
    "                    except:\n",
    "                        is_valid = False\n",
    "            else:\n",
    "                # For other columns\n",
    "                if pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                    is_valid = True\n",
    "                else:\n",
    "                    try:\n",
    "                        connections_df[col].astype(expected_type)\n",
    "                        is_valid = True\n",
    "                    except:\n",
    "                        is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            # Show sample of values to verify\n",
    "            print(f\"    Sample values: {connections_df[col].head(3).tolist()}\")\n",
    "            \n",
    "            # For ID columns, analyze value types\n",
    "            if col in [':START_ID', ':END_ID']:\n",
    "                num_values = connections_df[col].count()\n",
    "                num_numeric = connections_df[col].apply(lambda x: isinstance(x, (int, float)) or \n",
    "                                                      (isinstance(x, str) and x.isdigit())).sum()\n",
    "                num_string = num_values - num_numeric\n",
    "                \n",
    "                print(f\"    Value type breakdown: {num_numeric} numeric ({num_numeric/num_values:.2%}), \"\n",
    "                      f\"{num_string} non-numeric ({num_string/num_values:.2%})\")\n",
    "    \n",
    "    # ===== 2. MISSING VALUES VALIDATION =====\n",
    "    print(\"\\n===== MISSING VALUES VALIDATION =====\")\n",
    "    \n",
    "    # Check for missing values in key fields\n",
    "    print(\"\\nConcept DataFrame Missing Values in Key Fields:\")\n",
    "    missing_concept = concept_df['cui:ID'].isnull().sum()\n",
    "    print(f\"  - Primary key 'cui:ID': {missing_concept} missing values ({missing_concept/len(concept_df):.2%})\")\n",
    "    \n",
    "    print(\"\\nPredication DataFrame Missing Values in Key Fields:\")\n",
    "    missing_pred_id = predication_df['predication_id:ID'].isnull().sum()\n",
    "    missing_subject = predication_df['subject_cui:STRING'].isnull().sum()\n",
    "    missing_object = predication_df['object_cui:STRING'].isnull().sum()\n",
    "    print(f\"  - Primary key 'predication_id:ID': {missing_pred_id} missing values ({missing_pred_id/len(predication_df):.2%})\")\n",
    "    print(f\"  - Foreign key 'subject_cui:STRING': {missing_subject} missing values ({missing_subject/len(predication_df):.2%})\")\n",
    "    print(f\"  - Foreign key 'object_cui:STRING': {missing_object} missing values ({missing_object/len(predication_df):.2%})\")\n",
    "    \n",
    "    print(\"\\nConnections DataFrame Missing Values in Key Fields:\")\n",
    "    missing_src = connections_df[':START_ID'].isnull().sum()\n",
    "    missing_dest = connections_df[':END_ID'].isnull().sum()\n",
    "    missing_label = connections_df[':TYPE'].isnull().sum()\n",
    "    print(f\"  - ':START_ID': {missing_src} missing values ({missing_src/len(connections_df):.2%})\")\n",
    "    print(f\"  - ':END_ID': {missing_dest} missing values ({missing_dest/len(connections_df):.2%})\")\n",
    "    print(f\"  - ':TYPE': {missing_label} missing values ({missing_label/len(connections_df):.2%})\")\n",
    "    \n",
    "    # ===== 3. NODE UNIQUENESS VALIDATION =====\n",
    "    print(\"\\n===== NODE UNIQUENESS VALIDATION =====\")\n",
    "    \n",
    "    # Check for duplicate CUIs in concept_df\n",
    "    duplicate_cuis = concept_df['cui:ID'].duplicated().sum()\n",
    "    print(f\"Duplicate CUIs in concept_df: {duplicate_cuis} ({duplicate_cuis/len(concept_df):.2%})\")\n",
    "    \n",
    "    # Check for duplicate PREDICATION_IDs in predication_df\n",
    "    duplicate_preds = predication_df['predication_id:ID'].duplicated().sum()\n",
    "    print(f\"Duplicate PREDICATION_IDs in predication_df: {duplicate_preds} ({duplicate_preds/len(predication_df):.2%})\")\n",
    "    \n",
    "    # ===== 4. RELATIONSHIP INTEGRITY VALIDATION =====\n",
    "    print(\"\\n===== RELATIONSHIP INTEGRITY VALIDATION =====\")\n",
    "    \n",
    "    # Get distinct node IDs from the respective dataframes\n",
    "    concept_ids = set(concept_df['cui:ID'].dropna().astype(str))\n",
    "    predication_ids = set(predication_df['predication_id:ID'].dropna().astype(str))\n",
    "    \n",
    "    # All valid node IDs (combined)\n",
    "    all_valid_nodes = concept_ids.union(predication_ids)\n",
    "    \n",
    "    # Ensure IDs are strings for comparison\n",
    "    connections_df['start_id_str'] = connections_df[':START_ID'].astype(str)\n",
    "    connections_df['end_id_str'] = connections_df[':END_ID'].astype(str)\n",
    "    \n",
    "    # Check if relationship src_nodes exist\n",
    "    src_nodes = set(connections_df['start_id_str'].dropna())\n",
    "    invalid_src_nodes = src_nodes - all_valid_nodes\n",
    "    invalid_src_count = len(invalid_src_nodes)\n",
    "    \n",
    "    print(f\"\\nInvalid source nodes in relationships: {invalid_src_count} ({invalid_src_count/len(src_nodes):.2%})\")\n",
    "    if invalid_src_count > 0 and invalid_src_count <= 10:\n",
    "        print(f\"  Sample of invalid source nodes: {list(invalid_src_nodes)[:10]}\")\n",
    "    \n",
    "    # Check if relationship dest_nodes exist\n",
    "    dest_nodes = set(connections_df['end_id_str'].dropna())\n",
    "    invalid_dest_nodes = dest_nodes - all_valid_nodes\n",
    "    invalid_dest_count = len(invalid_dest_nodes)\n",
    "    \n",
    "    print(f\"\\nInvalid destination nodes in relationships: {invalid_dest_count} ({invalid_dest_count/len(dest_nodes):.2%})\")\n",
    "    if invalid_dest_count > 0 and invalid_dest_count <= 10:\n",
    "        print(f\"  Sample of invalid destination nodes: {list(invalid_dest_nodes)[:10]}\")\n",
    "    \n",
    "    # ===== 5. SUMMARY =====\n",
    "    print(\"\\n===== VALIDATION SUMMARY =====\")\n",
    "    \n",
    "    # Count total issues\n",
    "    total_issues = (missing_concept + missing_pred_id + missing_subject + missing_object + \n",
    "                    missing_src + missing_dest + missing_label +\n",
    "                    duplicate_cuis + duplicate_preds +\n",
    "                    invalid_src_count + invalid_dest_count)\n",
    "    \n",
    "    if total_issues == 0:\n",
    "        print(\"✅ All validations passed! Data appears clean and ready for Neo4j import.\")\n",
    "    else:\n",
    "        print(f\"❌ Found {total_issues} total issues that may affect your Neo4j import.\")\n",
    "        \n",
    "        # Recommend fixes based on issues found\n",
    "        print(\"\\nRecommended fixes:\")\n",
    "        \n",
    "        if missing_concept + missing_pred_id > 0:\n",
    "            print(\"  - Remove rows with missing primary keys (cui:ID or predication_id:ID)\")\n",
    "        \n",
    "        if missing_subject + missing_object > 0:\n",
    "            print(\"  - Fix or remove predications with missing subject_cui:STRING or object_cui:STRING\")\n",
    "        \n",
    "        if missing_src + missing_dest + missing_label > 0:\n",
    "            print(\"  - Remove relationships with missing :START_ID, :END_ID, or :TYPE\")\n",
    "        \n",
    "        if duplicate_cuis > 0:\n",
    "            print(\"  - Remove duplicate CUIs or merge their properties\")\n",
    "        \n",
    "        if duplicate_preds > 0:\n",
    "            print(\"  - Remove duplicate predication_id:IDs\")\n",
    "        \n",
    "        if invalid_src_count + invalid_dest_count > 0:\n",
    "            print(\"  - Remove relationships with invalid source or destination nodes\")\n",
    "            \n",
    "        # Check if START_ID and END_ID need type conversion\n",
    "        if not pd.api.types.is_string_dtype(connections_df[':START_ID'].dtype) or \\\n",
    "           not pd.api.types.is_string_dtype(connections_df[':END_ID'].dtype):\n",
    "            print(\"  - Convert ':START_ID' and ':END_ID' columns to string type (see fix_connections_dtypes function)\")\n",
    "\n",
    "def fix_connections_dtypes(file_path=\"connections.csv\"):\n",
    "    \"\"\"\n",
    "    Fix data types in the connections dataframe:\n",
    "    - Convert :START_ID and :END_ID to string type\n",
    "    - Save the fixed version\n",
    "    \"\"\"\n",
    "    # Load the connections CSV\n",
    "    connections_df = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "    # Convert src_node and dest_node to string\n",
    "    connections_df[':START_ID'] = connections_df[':START_ID'].astype(str)\n",
    "    connections_df[':END_ID'] = connections_df[':END_ID'].astype(str)\n",
    "    \n",
    "    # Save the fixed version\n",
    "    connections_df.to_csv(\"connections_fixed.csv\", index=False)\n",
    "    print(f\"Fixed connections data saved to 'connections_fixed.csv'\")\n",
    "    return connections_df\n",
    "\n",
    "validate_semmed_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
