{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrimeKG Processing and Validation Scripts\n",
    "To make it easier to load PrimeKG into Neo4j, I wrote some scripts to separate the nodes and edges. I have also verified uniqueness and made sure there are no dangling edges. The unique identifier for nodes is the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV data\n",
    "csv_file_path = 'data/kg.csv'\n",
    "data = pd.read_csv(csv_file_path, low_memory=False)\n",
    "\n",
    "# Create unique edges DataFrame with relation information\n",
    "unique_edges = data[['relation', 'display_relation', 'x_index', 'y_index']].copy()\n",
    "unique_edges_file_path = 'data/unique_edges.csv'\n",
    "unique_edges.to_csv(unique_edges_file_path, index=False)\n",
    "\n",
    "# Create unique nodes DataFrame\n",
    "# For x nodes\n",
    "x_nodes = data[['x_index', 'x_id', 'x_type', 'x_name']].drop_duplicates(subset=['x_index'])\n",
    "x_nodes.columns = ['index', 'node_id', 'node_type', 'node_name']\n",
    "\n",
    "# For y nodes\n",
    "y_nodes = data[['y_index', 'y_id', 'y_type', 'y_name']].drop_duplicates(subset=['y_index'])\n",
    "y_nodes.columns = ['index', 'node_id', 'node_type', 'node_name']\n",
    "\n",
    "# Combine x and y nodes and remove duplicates based on all columns\n",
    "unique_nodes = pd.concat([x_nodes, y_nodes])\n",
    "unique_nodes = unique_nodes.drop_duplicates(subset=['node_id', 'node_type', 'node_name'])\n",
    "unique_nodes_file_path = 'data/unique_nodes.csv'\n",
    "unique_nodes.to_csv(unique_nodes_file_path, index=False)\n",
    "\n",
    "# Print statistics\n",
    "print(f'Total relationships: {len(unique_edges)}')\n",
    "print(f'Total unique nodes: {len(unique_nodes)}')\n",
    "print(f'Unique node types: {unique_nodes[\"node_type\"].unique()}')\n",
    "\n",
    "# Verification\n",
    "print('\\nVerification counts by node type:')\n",
    "print(unique_nodes['node_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unique edges data\n",
    "edges_file_path = 'data/unique_edges.csv'\n",
    "edges_data = pd.read_csv(edges_file_path)\n",
    "\n",
    "# Load the unique nodes data\n",
    "nodes_file_path = 'data/unique_nodes.csv'\n",
    "nodes_data = pd.read_csv(nodes_file_path)\n",
    "\n",
    "# Get and display relationship statistics\n",
    "print(\"\\nRelationship Type Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Count unique relationships\n",
    "relationship_counts = edges_data['relation'].value_counts()\n",
    "display_relation_counts = edges_data['display_relation'].value_counts()\n",
    "\n",
    "print(f\"\\nTotal unique relationship types: {len(relationship_counts)}\")\n",
    "print(\"\\nRelationship counts:\")\n",
    "print(\"-\" * 50)\n",
    "for rel, count in relationship_counts.items():\n",
    "    display_rel = edges_data[edges_data['relation'] == rel]['display_relation'].iloc[0]\n",
    "    print(f\"{rel} ({display_rel}): {count} occurrences\")\n",
    "\n",
    "# Save relationship statistics to CSV\n",
    "relationship_stats = pd.DataFrame({\n",
    "    'relation': relationship_counts.index,\n",
    "    'count': relationship_counts.values\n",
    "}).merge(\n",
    "    edges_data[['relation', 'display_relation']].drop_duplicates(),\n",
    "    on='relation'\n",
    ")\n",
    "\n",
    "stats_file_path = 'data/relationship_stats.csv'\n",
    "relationship_stats.to_csv(stats_file_path, index=False)\n",
    "print(f\"\\nDetailed statistics saved to: {stats_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation section\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KNOWLEDGE GRAPH VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Check for duplicate nodes\n",
    "print(\"\\n1. Checking for duplicate nodes...\")\n",
    "duplicate_nodes = nodes_data[nodes_data.duplicated(subset=['index'])]\n",
    "if len(duplicate_nodes) > 0:\n",
    "    print(f\"WARNING: Found {len(duplicate_nodes)} duplicate node indices!\")\n",
    "    print(duplicate_nodes)\n",
    "    \n",
    "    # Remove duplicate nodes\n",
    "    print(\"\\nRemoving duplicate node indices...\")\n",
    "    original_count = len(nodes_data)\n",
    "    nodes_data = nodes_data.drop_duplicates(subset=['index'])\n",
    "    print(f\"✓ Removed {original_count - len(nodes_data)} duplicate node indices.\")\n",
    "else:\n",
    "    print(\"✓ No duplicate node indices found.\")\n",
    "\n",
    "# Also check for duplicate node IDs\n",
    "duplicate_node_ids = nodes_data[nodes_data.duplicated(subset=['node_id'])]\n",
    "if len(duplicate_node_ids) > 0:\n",
    "    print(f\"WARNING: Found {len(duplicate_node_ids)} duplicate node IDs!\")\n",
    "    print(duplicate_node_ids)\n",
    "    \n",
    "    # Remove duplicate node IDs\n",
    "    print(\"\\nRemoving duplicate node IDs...\")\n",
    "    original_count = len(nodes_data)\n",
    "    nodes_data = nodes_data.drop_duplicates(subset=['node_id'])\n",
    "    print(f\"✓ Removed {original_count - len(nodes_data)} duplicate node IDs.\")\n",
    "else:\n",
    "    print(\"✓ No duplicate node IDs found.\")\n",
    "\n",
    "# 2. Verify that all relationship endpoints exist in the nodes dataset\n",
    "print(\"\\n2. Verifying relationship endpoints...\")\n",
    "node_indices = set(nodes_data['index'].values)\n",
    "\n",
    "# Check x_index (source nodes)\n",
    "missing_source_nodes = edges_data[~edges_data['x_index'].isin(node_indices)]\n",
    "if len(missing_source_nodes) > 0:\n",
    "    print(f\"WARNING: Found {len(missing_source_nodes)} relationships with missing source nodes!\")\n",
    "    print(missing_source_nodes.head(10))  # Show first 10 examples\n",
    "else:\n",
    "    print(\"✓ All relationship source nodes exist in the nodes dataset.\")\n",
    "\n",
    "# Check y_index (target nodes)\n",
    "missing_target_nodes = edges_data[~edges_data['y_index'].isin(node_indices)]\n",
    "if len(missing_target_nodes) > 0:\n",
    "    print(f\"WARNING: Found {len(missing_target_nodes)} relationships with missing target nodes!\")\n",
    "    print(missing_target_nodes.head(10))  # Show first 10 examples\n",
    "else:\n",
    "    print(\"✓ All relationship target nodes exist in the nodes dataset.\")\n",
    "\n",
    "# 3. Summary\n",
    "print(\"\\nValidation Summary:\")\n",
    "if len(duplicate_nodes) == 0 and len(duplicate_node_ids) == 0 and len(missing_source_nodes) == 0 and len(missing_target_nodes) == 0:\n",
    "    print(\"✓ Knowledge graph validation passed. No issues found.\")\n",
    "else:\n",
    "    total_issues = len(duplicate_nodes) + len(duplicate_node_ids) + len(missing_source_nodes) + len(missing_target_nodes)\n",
    "    print(f\"⚠ Knowledge graph validation found {total_issues} issues that should be addressed.\")\n",
    "\n",
    "# Save the cleaned nodes data\n",
    "if len(duplicate_nodes) > 0 or len(duplicate_node_ids) > 0:\n",
    "    cleaned_nodes_file_path = 'data/unique_nodes_cleaned.csv'\n",
    "    nodes_data.to_csv(cleaned_nodes_file_path, index=False)\n",
    "    print(f\"\\nCleaned nodes data saved to: {cleaned_nodes_file_path}\")\n",
    "    print(\"NOTE: Please review the cleaned data and replace 'data/unique_nodes.csv' with it if satisfied.\")\n",
    "\n",
    "# Optional: Save validation results to a file\n",
    "validation_results = {\n",
    "    'duplicate_node_indices': len(duplicate_nodes),\n",
    "    'duplicate_node_ids': len(duplicate_node_ids),\n",
    "    'missing_source_nodes': len(missing_source_nodes),\n",
    "    'missing_target_nodes': len(missing_target_nodes)\n",
    "}\n",
    "\n",
    "validation_df = pd.DataFrame([validation_results])\n",
    "validation_file_path = 'data/kg_validation_results.csv'\n",
    "validation_df.to_csv(validation_file_path, index=False)\n",
    "print(f\"\\nDetailed validation results saved to: {validation_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unique nodes data\n",
    "nodes_file_path = 'data/unique_nodes.csv'\n",
    "nodes_data = pd.read_csv(nodes_file_path)\n",
    "\n",
    "# Extract unique node types and save to text file\n",
    "unique_node_types = nodes_data['node_type:string'].unique()\n",
    "node_types_file_path = 'data/unique_node_types.txt'\n",
    "\n",
    "# Write the unique node types to a text file, one per line\n",
    "with open(node_types_file_path, 'w') as f:\n",
    "    for node_type in unique_node_types:\n",
    "        f.write(f\"{node_type}\\n\")\n",
    "\n",
    "print(f\"\\nUnique node types saved to: {node_types_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
