{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Headers for original predication and predication aux files. \n",
    "predication_headers = [\n",
    "    'PREDICATION_ID', 'SENTENCE_ID', 'PMID', 'PREDICATE', 'SUBJECT_CUI',\n",
    "    'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY', 'OBJECT_CUI',\n",
    "    'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY', 'FACT_VALUE_CHAR',\n",
    "    'MOD_SCALE_CHAR', 'MOD_VALUE_FLOAT'\n",
    "]\n",
    "\n",
    "predication_aux_headers = [\n",
    "    'PREDICATION_AUX_ID', 'PREDICATION_ID', 'SUBJECT_TEXT', 'SUBJECT_DIST',\n",
    "    'SUBJECT_MAXDIST', 'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE',\n",
    "    'INDICATOR_TYPE', 'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX', 'OBJECT_TEXT',\n",
    "    'OBJECT_DIST', 'OBJECT_MAXDIST', 'OBJECT_START_INDEX', 'OBJECT_END_INDEX',\n",
    "    'OBJECT_SCORE', 'CURR_TIMESTAMP'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Nodes\n",
    "The following code will create two CSVs for entity and predication nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original CSV files\n",
    "df = pd.read_csv('semmed_data/predication.csv', names=predication_headers, encoding='ISO-8859-1', on_bad_lines='warn', na_values=['\\\\N'])\n",
    "\n",
    "df_aux = pd.read_csv('semmed_data/predication_aux.csv', names=predication_aux_headers, encoding='ISO-8859-1', on_bad_lines='warn', na_values=['\\\\N']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, df_aux, on='PREDICATION_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predication_df from both main and aux dataframes\n",
    "predication_base_columns = ['PREDICATION_ID', 'SENTENCE_ID', 'PMID', 'PREDICATE',\n",
    "                          'SUBJECT_CUI', 'OBJECT_CUI']\n",
    "predication_aux_columns = ['PREDICATION_ID', 'INDICATOR_TYPE', \n",
    "                         'PREDICATE_START_INDEX', 'PREDICATE_END_INDEX']\n",
    "\n",
    "# Get base predication info\n",
    "predication_df = df[predication_base_columns].copy()\n",
    "\n",
    "# Get auxiliary info and merge\n",
    "aux_info = df_aux[predication_aux_columns].copy()\n",
    "predication_df = predication_df.merge(aux_info, on='PREDICATION_ID', how='left')\n",
    "aux_info = aux_info.rename(columns={'PREDICATION_ID': 'PREDICATION_ID:ID'})\n",
    "predication_df = predication_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predication_df to CSV with each field in quotes\n",
    "predication_df.to_csv('data/predication.csv', index=False, quoting=csv.QUOTE_ALL, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create concept_df from main and aux dataframes\n",
    "subject_base_columns = ['PREDICATION_ID', 'SUBJECT_CUI', 'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY']\n",
    "subject_base = df[subject_base_columns].copy()\n",
    "\n",
    "# Subject columns from aux df\n",
    "subject_aux_columns = ['PREDICATION_ID', 'SUBJECT_TEXT', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', \n",
    "                      'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE']\n",
    "subject_aux = df_aux[subject_aux_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object columns from predication df\n",
    "object_base_columns = ['PREDICATION_ID', 'OBJECT_CUI', 'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY']\n",
    "object_base = df[object_base_columns].copy()\n",
    "\n",
    "# Object columns from aux df\n",
    "object_aux_columns = ['PREDICATION_ID', 'OBJECT_TEXT', 'OBJECT_DIST', 'OBJECT_MAXDIST', \n",
    "                     'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE']\n",
    "object_aux = df_aux[object_aux_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge base and aux for subjects and objects\n",
    "subject_entities = subject_base.merge(subject_aux, on='PREDICATION_ID').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_entities = object_base.merge(object_aux, on='PREDICATION_ID').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to prepare for merging\n",
    "concept_columns = ['CUI:ID', 'NAME', 'SEMTYPE', 'NOVELTY', 'TEXT', \n",
    "                  'DIST', 'MAXDIST', 'START_INDEX', 'END_INDEX', 'SCORE']\n",
    "subject_entities.columns = concept_columns\n",
    "object_entities.columns = concept_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine subject and object entities and remove duplicates based on CUI\n",
    "concept_df = pd.concat([subject_entities, object_entities]).drop_duplicates(subset=['CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = concept_df.drop(columns=['PREDICATION_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_columns = ['SUBJECT_CUI', 'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY',\n",
    "                  'SUBJECT_TEXT', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', \n",
    "                  'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE']\n",
    "\n",
    "# Extract subject entities\n",
    "subject_entities = merged_df[subject_columns].drop_duplicates()\n",
    "\n",
    "# Rename columns to prepare for merging with object entities\n",
    "concept_columns = ['CUI', 'NAME', 'SEMTYPE', 'NOVELTY', 'TEXT', \n",
    "                 'DIST', 'MAXDIST', 'START_INDEX', 'END_INDEX', 'SCORE']\n",
    "\n",
    "subject_entities.columns = concept_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_columns = ['SUBJECT_CUI', 'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'SUBJECT_NOVELTY',\n",
    "                  'SUBJECT_TEXT', 'SUBJECT_DIST', 'SUBJECT_MAXDIST', \n",
    "                  'SUBJECT_START_INDEX', 'SUBJECT_END_INDEX', 'SUBJECT_SCORE']\n",
    "\n",
    "# Extract subject entities\n",
    "subject_entities = merged_df[subject_columns].drop_duplicates()\n",
    "\n",
    "# Rename columns to prepare for merging with object entities\n",
    "concept_columns = ['CUI', 'NAME', 'SEMTYPE', 'NOVELTY', 'TEXT', \n",
    "                 'DIST', 'MAXDIST', 'START_INDEX', 'END_INDEX', 'SCORE']\n",
    "\n",
    "subject_entities.columns = concept_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract object entities using the same structure\n",
    "object_columns = ['OBJECT_CUI', 'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY',\n",
    "                 'OBJECT_TEXT', 'OBJECT_DIST', 'OBJECT_MAXDIST', \n",
    "                 'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE']\n",
    "\n",
    "object_entities = merged_df[object_columns].drop_duplicates()\n",
    "object_entities.columns = concept_columns\n",
    "\n",
    "# Combine subject and object entities and remove duplicates based on CUI\n",
    "concept_df = pd.concat([subject_entities, object_entities]).drop_duplicates(subset=['CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df.to_csv('data/concept.csv', index=False, quoting=csv.QUOTE_ALL, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract object entities using the same structure\n",
    "object_columns = ['object_cui:STRING', 'OBJECT_NAME', 'OBJECT_SEMTYPE', 'OBJECT_NOVELTY',\n",
    "                 'OBJECT_TEXT', 'OBJECT_DIST', 'OBJECT_MAXDIST', \n",
    "                 'OBJECT_START_INDEX', 'OBJECT_END_INDEX', 'OBJECT_SCORE']\n",
    "\n",
    "object_entities = merged_df[object_columns].drop_duplicates()\n",
    "object_entities.columns = concept_columns\n",
    "\n",
    "# Combine subject and object entities and remove duplicates based on CUI\n",
    "concept_df = pd.concat([subject_entities, object_entities]).drop_duplicates(subset=['CUI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predication dataframe shape: {predication_df.shape}\")\n",
    "print(f\"Entity dataframe shape: {concept_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df.to_csv(\"predication.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df.to_csv(\"concept.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Relationships\n",
    "The following code will create a CSV with all the connections between the concepts and predicates in a format that is easily digestible by Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df = pd.read_csv(\"data/predication.csv\", names=['PREDICATION_ID','SENTENCE_ID','PMID','PREDICATE','SUBJECT_CUI','OBJECT_CUI','INDICATOR_TYPE','PREDICATE_START_INDEX','PREDICATE_END_INDEX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for connections\n",
    "connections_columns = ['src_node', 'dest_node', 'label']\n",
    "connections_df = pd.DataFrame(columns=connections_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Connections between predication instances and subjects (inst_subject)\n",
    "inst_subject_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['PREDICATION_ID'],\n",
    "    'dest_node': predication_df['SUBJECT_CUI'],\n",
    "    'label': 'inst_subject'\n",
    "})\n",
    "\n",
    "# 2. Connections between predication instances and objects (inst_object)\n",
    "inst_object_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['PREDICATION_ID'],\n",
    "    'dest_node': predication_df['OBJECT_CUI'],\n",
    "    'label': 'inst_object'\n",
    "})\n",
    "\n",
    "# 3. Connections between subjects and objects (using PREDICATE as the label)\n",
    "subject_object_connections = pd.DataFrame({\n",
    "    'src_node': predication_df['SUBJECT_CUI'],\n",
    "    'dest_node': predication_df['OBJECT_CUI'],\n",
    "    'label': predication_df['PREDICATE']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all connections into the final connections dataframe\n",
    "connections_df = pd.concat([\n",
    "    inst_subject_connections,\n",
    "    inst_object_connections,\n",
    "    subject_object_connections\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index for the final dataframe\n",
    "connections_df = connections_df.reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Connections dataframe shape: {connections_df.shape}\")\n",
    "print(connections_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_df.to_csv(\"connections.csv\", index=False, quoting=csv.QUOTE_ALL, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Structure\n",
    "The Neo4j import requires that you format your CSV with the datatypes in the header so here I am re-labeling the CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For entity/concept file\n",
    "# concept_df = pd.read_csv(\"data/concept.csv\", index_col=0)\n",
    "# concept_df.columns = [\n",
    "#     \"cui\", \n",
    "#     \"name:STRING\", \n",
    "#     \"semtype:LABEL\", \n",
    "#     \"novelty:FLOAT\", \n",
    "#     \"text:STRING\", \n",
    "#     \"dist\", \n",
    "#     \"maxdist\", \n",
    "#     \"start_index\", \n",
    "#     \"end_index\", \n",
    "#     \"score\", \n",
    "#     \"uuid\"\n",
    "# ]\n",
    "# concept_df.to_csv(\"entity_neo4j.csv\", index=False)\n",
    "\n",
    "# For predication file - now with predicate as LABEL\n",
    "predication_df = pd.read_csv(\"data/predication.csv\")\n",
    "predication_df.columns = [\n",
    "    \"predication_id:ID\", \n",
    "    \"sentence_id\", \n",
    "    \"pmid:STRING\", \n",
    "    \"predicate:LABEL\",  # Changed from STRING to LABEL\n",
    "    \"subject_cui:STRING\", \n",
    "    \"object_cui:STRING\", \n",
    "    \"indicator_type:STRING\", \n",
    "    \"predicate_start_index\", \n",
    "    \"predicate_end_index\"\n",
    "]\n",
    "predication_df.to_csv(\"predication_neo4j.csv\", index=False)\n",
    "\n",
    "# For connections/relationships file\n",
    "connections_df = pd.read_csv(\"data/connections.csv\")\n",
    "connections_df.columns = [\n",
    "    \":START_ID\", \n",
    "    \":END_ID\", \n",
    "    \":TYPE\"\n",
    "]\n",
    "connections_df.to_csv(\"relationships_neo4j.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.read_csv(\"concept.csv\")\n",
    "predication_df = pd.read_csv(\"predication.csv\")\n",
    "connections_df = pd.read_csv(\"connections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of nodes, relationships and properties\n",
    "print(\"\\n=== GRAPH STATISTICS ===\")\n",
    "\n",
    "# Count of nodes by type\n",
    "print(\"\\nNODE COUNTS:\")\n",
    "print(f\"Concept/Entity nodes: {len(concept_df)}\")\n",
    "print(f\"Predication nodes: {len(predication_df)}\")\n",
    "print(f\"Total nodes: {len(concept_df) + len(predication_df)}\")\n",
    "\n",
    "# Count of relationships by type\n",
    "print(\"\\nRELATIONSHIP COUNTS:\")\n",
    "relationship_counts = connections_df[':TYPE'].value_counts()\n",
    "print(\"Top 10 relationship types:\")\n",
    "print(relationship_counts.head(10))\n",
    "print(f\"Total relationships: {len(connections_df)}\")\n",
    "\n",
    "# Count of unique predicates\n",
    "print(\"\\nUNIQUE PREDICATES:\")\n",
    "unique_predicates = predication_df['predicate:LABEL'].nunique()\n",
    "print(f\"Number of unique predicates: {unique_predicates}\")\n",
    "print(\"Most common predicates:\")\n",
    "print(predication_df['predicate:LABEL'].value_counts().head(10))\n",
    "\n",
    "# Property statistics\n",
    "print(\"\\nPROPERTY STATISTICS:\")\n",
    "print(\"Entity properties:\")\n",
    "for col in concept_df.columns:\n",
    "    prop_name = col.split(':')[0]\n",
    "    non_null = concept_df[col].count()\n",
    "    print(f\"  - {prop_name}: {non_null} non-null values ({non_null/len(concept_df):.2%} coverage)\")\n",
    "\n",
    "print(\"\\nPredication properties:\")\n",
    "for col in predication_df.columns:\n",
    "    prop_name = col.split(':')[0]\n",
    "    non_null = predication_df[col].count()\n",
    "    print(f\"  - {prop_name}: {non_null} non-null values ({non_null/len(predication_df):.2%} coverage)\")\n",
    "\n",
    "# Graph density analysis\n",
    "print(\"\\nGRAPH DENSITY ANALYSIS:\")\n",
    "num_nodes = len(concept_df) + len(predication_df)\n",
    "num_edges = len(connections_df)\n",
    "max_possible_edges = num_nodes * (num_nodes - 1) / 2  # for undirected graph\n",
    "graph_density = num_edges / max_possible_edges\n",
    "print(f\"Graph density: {graph_density:.8f}\")\n",
    "\n",
    "# Distribution of connections per node\n",
    "print(\"\\nCONNECTION DISTRIBUTION:\")\n",
    "src_connections = connections_df[':START_ID'].value_counts()\n",
    "dest_connections = connections_df[':END_ID'].value_counts()\n",
    "\n",
    "print(\"Source node connection statistics:\")\n",
    "print(f\"  - Mean connections per node: {src_connections.mean():.2f}\")\n",
    "print(f\"  - Median connections per node: {src_connections.median():.2f}\")\n",
    "print(f\"  - Max connections: {src_connections.max()}\")\n",
    "\n",
    "print(\"Destination node connection statistics:\")\n",
    "print(f\"  - Mean connections per node: {dest_connections.mean():.2f}\")\n",
    "print(f\"  - Median connections per node: {dest_connections.median():.2f}\")\n",
    "print(f\"  - Max connections: {dest_connections.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_semmed_data():\n",
    "    \"\"\"\n",
    "    Validate the SemMedDB processed data for Neo4j import:\n",
    "    1. Check data types for all columns\n",
    "    2. Verify node uniqueness (no duplicates)\n",
    "    3. Verify relationship integrity (start/end nodes exist)\n",
    "    4. Check for missing values in key fields\n",
    "    \"\"\"\n",
    "    print(\"Starting SemMedDB data validation...\\n\")\n",
    "    \n",
    "    # Load the three dataframes with low_memory=False to avoid dtype warnings\n",
    "    print(\"Loading data...\")\n",
    "    concept_df = pd.read_csv(\"concept.csv\", low_memory=False)\n",
    "    predication_df = pd.read_csv(\"predication.csv\", low_memory=False)\n",
    "    connections_df = pd.read_csv(\"connections.csv\", low_memory=False)\n",
    "    \n",
    "    # ===== 1. DATA TYPES VALIDATION =====\n",
    "    print(\"\\n===== DATA TYPES VALIDATION =====\")\n",
    "    \n",
    "    # Expected data types for concept_df\n",
    "    concept_dtypes = {\n",
    "        'cui:ID': str,\n",
    "        'name:STRING': str,\n",
    "        'semtype:LABEL': str,\n",
    "        'novelty:FLOAT': float,\n",
    "        'text:STRING': str,\n",
    "        'dist:INTEGER': int,\n",
    "        'maxdist:INTEGER': int,\n",
    "        'start_index:INTEGER': int,\n",
    "        'end_index:INTEGER': int,\n",
    "        'score:INTEGER': int\n",
    "    }\n",
    "    \n",
    "    # Expected data types for predication_df\n",
    "    predication_dtypes = {\n",
    "        'predication_id:ID': int,\n",
    "        'sentence_id:INTEGER': int,\n",
    "        'pmid:STRING': str,\n",
    "        'predicate:LABEL': str,\n",
    "        'subject_cui:STRING': str,\n",
    "        'object_cui:STRING': str,\n",
    "        'indicator_type:STRING': str,\n",
    "        'predicate_start_index:INTEGER': int,\n",
    "        'predicate_end_index:INTEGER': int\n",
    "    }\n",
    "    \n",
    "    # Expected data types for connections_df\n",
    "    connections_dtypes = {\n",
    "        ':START_ID': str,\n",
    "        ':END_ID': str,\n",
    "        ':TYPE': str\n",
    "    }\n",
    "    \n",
    "    # Check data types for concept_df\n",
    "    print(\"\\nChecking concept_df data types:\")\n",
    "    for col, expected_type in concept_dtypes.items():\n",
    "        if col in concept_df.columns:\n",
    "            # Get actual type and handle mixed types\n",
    "            actual_type = concept_df[col].dtype\n",
    "            if pd.api.types.is_numeric_dtype(actual_type) and expected_type in [int, float]:\n",
    "                is_valid = True\n",
    "            elif pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                is_valid = True\n",
    "            else:\n",
    "                # For mixed types, check if conversion is possible\n",
    "                try:\n",
    "                    concept_df[col].astype(expected_type)\n",
    "                    is_valid = True\n",
    "                except:\n",
    "                    is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            if not is_valid:\n",
    "                # Show sample of problematic values\n",
    "                print(f\"    Sample values: {concept_df[col].head(3).tolist()}\")\n",
    "                \n",
    "                # Try to identify specific issues\n",
    "                if expected_type == int:\n",
    "                    non_int_mask = ~concept_df[col].astype(str).str.match(r'^-?\\d+$', na=False)\n",
    "                    print(f\"    Problematic non-integer values: {concept_df.loc[non_int_mask, col].head(3).tolist()}\")\n",
    "    \n",
    "    # Check data types for predication_df\n",
    "    print(\"\\nChecking predication_df data types:\")\n",
    "    for col, expected_type in predication_dtypes.items():\n",
    "        if col in predication_df.columns:\n",
    "            actual_type = predication_df[col].dtype\n",
    "            if pd.api.types.is_numeric_dtype(actual_type) and expected_type in [int, float]:\n",
    "                is_valid = True\n",
    "            elif pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                is_valid = True\n",
    "            else:\n",
    "                try:\n",
    "                    predication_df[col].astype(expected_type)\n",
    "                    is_valid = True\n",
    "                except:\n",
    "                    is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            if not is_valid:\n",
    "                print(f\"    Sample values: {predication_df[col].head(3).tolist()}\")\n",
    "                if expected_type == int:\n",
    "                    non_int_mask = ~predication_df[col].astype(str).str.match(r'^-?\\d+$', na=False)\n",
    "                    print(f\"    Problematic non-integer values: {predication_df.loc[non_int_mask, col].head(3).tolist()}\")\n",
    "    \n",
    "    # Check data types for connections_df\n",
    "    print(\"\\nChecking connections_df data types:\")\n",
    "    for col, expected_type in connections_dtypes.items():\n",
    "        if col in connections_df.columns:\n",
    "            actual_type = connections_df[col].dtype\n",
    "            \n",
    "            # For ID columns, check if they are strings or can be converted to strings\n",
    "            if col in [':START_ID', ':END_ID']:\n",
    "                if pd.api.types.is_string_dtype(actual_type):\n",
    "                    is_valid = True\n",
    "                else:\n",
    "                    # Check if all values can be converted to string without issues\n",
    "                    try:\n",
    "                        connections_df[col].astype(str)\n",
    "                        is_valid = True\n",
    "                        print(f\"    Note: {col} will need conversion to string type\")\n",
    "                    except:\n",
    "                        is_valid = False\n",
    "            else:\n",
    "                # For other columns\n",
    "                if pd.api.types.is_string_dtype(actual_type) and expected_type == str:\n",
    "                    is_valid = True\n",
    "                else:\n",
    "                    try:\n",
    "                        connections_df[col].astype(expected_type)\n",
    "                        is_valid = True\n",
    "                    except:\n",
    "                        is_valid = False\n",
    "            \n",
    "            print(f\"  - {col}: Expected {expected_type.__name__}, Got {actual_type} - {'✓' if is_valid else '✗'}\")\n",
    "            \n",
    "            # Show sample of values to verify\n",
    "            print(f\"    Sample values: {connections_df[col].head(3).tolist()}\")\n",
    "            \n",
    "            # For ID columns, analyze value types\n",
    "            if col in [':START_ID', ':END_ID']:\n",
    "                num_values = connections_df[col].count()\n",
    "                num_numeric = connections_df[col].apply(lambda x: isinstance(x, (int, float)) or \n",
    "                                                      (isinstance(x, str) and x.isdigit())).sum()\n",
    "                num_string = num_values - num_numeric\n",
    "                \n",
    "                print(f\"    Value type breakdown: {num_numeric} numeric ({num_numeric/num_values:.2%}), \"\n",
    "                      f\"{num_string} non-numeric ({num_string/num_values:.2%})\")\n",
    "    \n",
    "    # ===== 2. MISSING VALUES VALIDATION =====\n",
    "    print(\"\\n===== MISSING VALUES VALIDATION =====\")\n",
    "    \n",
    "    # Check for missing values in key fields\n",
    "    print(\"\\nConcept DataFrame Missing Values in Key Fields:\")\n",
    "    missing_concept = concept_df['cui:ID'].isnull().sum()\n",
    "    print(f\"  - Primary key 'cui:ID': {missing_concept} missing values ({missing_concept/len(concept_df):.2%})\")\n",
    "    \n",
    "    print(\"\\nPredication DataFrame Missing Values in Key Fields:\")\n",
    "    missing_pred_id = predication_df['predication_id:ID'].isnull().sum()\n",
    "    missing_subject = predication_df['subject_cui:STRING'].isnull().sum()\n",
    "    missing_object = predication_df['object_cui:STRING'].isnull().sum()\n",
    "    print(f\"  - Primary key 'predication_id:ID': {missing_pred_id} missing values ({missing_pred_id/len(predication_df):.2%})\")\n",
    "    print(f\"  - Foreign key 'subject_cui:STRING': {missing_subject} missing values ({missing_subject/len(predication_df):.2%})\")\n",
    "    print(f\"  - Foreign key 'object_cui:STRING': {missing_object} missing values ({missing_object/len(predication_df):.2%})\")\n",
    "    \n",
    "    print(\"\\nConnections DataFrame Missing Values in Key Fields:\")\n",
    "    missing_src = connections_df[':START_ID'].isnull().sum()\n",
    "    missing_dest = connections_df[':END_ID'].isnull().sum()\n",
    "    missing_label = connections_df[':TYPE'].isnull().sum()\n",
    "    print(f\"  - ':START_ID': {missing_src} missing values ({missing_src/len(connections_df):.2%})\")\n",
    "    print(f\"  - ':END_ID': {missing_dest} missing values ({missing_dest/len(connections_df):.2%})\")\n",
    "    print(f\"  - ':TYPE': {missing_label} missing values ({missing_label/len(connections_df):.2%})\")\n",
    "    \n",
    "    # ===== 3. NODE UNIQUENESS VALIDATION =====\n",
    "    print(\"\\n===== NODE UNIQUENESS VALIDATION =====\")\n",
    "    \n",
    "    # Check for duplicate CUIs in concept_df\n",
    "    duplicate_cuis = concept_df['cui:ID'].duplicated().sum()\n",
    "    print(f\"Duplicate CUIs in concept_df: {duplicate_cuis} ({duplicate_cuis/len(concept_df):.2%})\")\n",
    "    \n",
    "    # Check for duplicate PREDICATION_IDs in predication_df\n",
    "    duplicate_preds = predication_df['predication_id:ID'].duplicated().sum()\n",
    "    print(f\"Duplicate PREDICATION_IDs in predication_df: {duplicate_preds} ({duplicate_preds/len(predication_df):.2%})\")\n",
    "    \n",
    "    # ===== 4. RELATIONSHIP INTEGRITY VALIDATION =====\n",
    "    print(\"\\n===== RELATIONSHIP INTEGRITY VALIDATION =====\")\n",
    "    \n",
    "    # Get distinct node IDs from the respective dataframes\n",
    "    concept_ids = set(concept_df['cui:ID'].dropna().astype(str))\n",
    "    predication_ids = set(predication_df['predication_id:ID'].dropna().astype(str))\n",
    "    \n",
    "    # All valid node IDs (combined)\n",
    "    all_valid_nodes = concept_ids.union(predication_ids)\n",
    "    \n",
    "    # Ensure IDs are strings for comparison\n",
    "    connections_df['start_id_str'] = connections_df[':START_ID'].astype(str)\n",
    "    connections_df['end_id_str'] = connections_df[':END_ID'].astype(str)\n",
    "    \n",
    "    # Check if relationship src_nodes exist\n",
    "    src_nodes = set(connections_df['start_id_str'].dropna())\n",
    "    invalid_src_nodes = src_nodes - all_valid_nodes\n",
    "    invalid_src_count = len(invalid_src_nodes)\n",
    "    \n",
    "    print(f\"\\nInvalid source nodes in relationships: {invalid_src_count} ({invalid_src_count/len(src_nodes):.2%})\")\n",
    "    if invalid_src_count > 0 and invalid_src_count <= 10:\n",
    "        print(f\"  Sample of invalid source nodes: {list(invalid_src_nodes)[:10]}\")\n",
    "    \n",
    "    # Check if relationship dest_nodes exist\n",
    "    dest_nodes = set(connections_df['end_id_str'].dropna())\n",
    "    invalid_dest_nodes = dest_nodes - all_valid_nodes\n",
    "    invalid_dest_count = len(invalid_dest_nodes)\n",
    "    \n",
    "    print(f\"\\nInvalid destination nodes in relationships: {invalid_dest_count} ({invalid_dest_count/len(dest_nodes):.2%})\")\n",
    "    if invalid_dest_count > 0 and invalid_dest_count <= 10:\n",
    "        print(f\"  Sample of invalid destination nodes: {list(invalid_dest_nodes)[:10]}\")\n",
    "    \n",
    "    # ===== 5. SUMMARY =====\n",
    "    print(\"\\n===== VALIDATION SUMMARY =====\")\n",
    "    \n",
    "    # Count total issues\n",
    "    total_issues = (missing_concept + missing_pred_id + missing_subject + missing_object + \n",
    "                    missing_src + missing_dest + missing_label +\n",
    "                    duplicate_cuis + duplicate_preds +\n",
    "                    invalid_src_count + invalid_dest_count)\n",
    "    \n",
    "    if total_issues == 0:\n",
    "        print(\"✅ All validations passed! Data appears clean and ready for Neo4j import.\")\n",
    "    else:\n",
    "        print(f\"❌ Found {total_issues} total issues that may affect your Neo4j import.\")\n",
    "        \n",
    "        # Recommend fixes based on issues found\n",
    "        print(\"\\nRecommended fixes:\")\n",
    "        \n",
    "        if missing_concept + missing_pred_id > 0:\n",
    "            print(\"  - Remove rows with missing primary keys (cui:ID or predication_id:ID)\")\n",
    "        \n",
    "        if missing_subject + missing_object > 0:\n",
    "            print(\"  - Fix or remove predications with missing subject_cui:STRING or object_cui:STRING\")\n",
    "        \n",
    "        if missing_src + missing_dest + missing_label > 0:\n",
    "            print(\"  - Remove relationships with missing :START_ID, :END_ID, or :TYPE\")\n",
    "        \n",
    "        if duplicate_cuis > 0:\n",
    "            print(\"  - Remove duplicate CUIs or merge their properties\")\n",
    "        \n",
    "        if duplicate_preds > 0:\n",
    "            print(\"  - Remove duplicate predication_id:IDs\")\n",
    "        \n",
    "        if invalid_src_count + invalid_dest_count > 0:\n",
    "            print(\"  - Remove relationships with invalid source or destination nodes\")\n",
    "            \n",
    "        # Check if START_ID and END_ID need type conversion\n",
    "        if not pd.api.types.is_string_dtype(connections_df[':START_ID'].dtype) or \\\n",
    "           not pd.api.types.is_string_dtype(connections_df[':END_ID'].dtype):\n",
    "            print(\"  - Convert ':START_ID' and ':END_ID' columns to string type (see fix_connections_dtypes function)\")\n",
    "\n",
    "def fix_connections_dtypes(file_path=\"connections.csv\"):\n",
    "    \"\"\"\n",
    "    Fix data types in the connections dataframe:\n",
    "    - Convert :START_ID and :END_ID to string type\n",
    "    - Save the fixed version\n",
    "    \"\"\"\n",
    "    # Load the connections CSV\n",
    "    connections_df = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "    # Convert src_node and dest_node to string\n",
    "    connections_df[':START_ID'] = connections_df[':START_ID'].astype(str)\n",
    "    connections_df[':END_ID'] = connections_df[':END_ID'].astype(str)\n",
    "    \n",
    "    # Save the fixed version\n",
    "    connections_df.to_csv(\"connections_fixed.csv\", index=False)\n",
    "    print(f\"Fixed connections data saved to 'connections_fixed.csv'\")\n",
    "    return connections_df\n",
    "\n",
    "validate_semmed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the :ID col to uuid instead of cui since some of them are compound cuis\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# Function to generate UUID\n",
    "def generate_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "# Read the files\n",
    "concept_df = pd.read_csv(\"data/concept.csv\")\n",
    "connections_df = pd.read_csv(\"data/connections.csv\")\n",
    "\n",
    "# Add UUID column to concept_df and rename CUI column\n",
    "concept_df[':ID'] = [generate_uuid() for _ in range(len(concept_df))]\n",
    "concept_df.rename(columns={'cui:ID': 'cui:STRING'}, inplace=True)\n",
    "\n",
    "# Create a mapping dictionary from CUI to UUID\n",
    "cui_to_uuid = dict(zip(concept_df['cui:STRING'], concept_df[':ID']))\n",
    "\n",
    "# Update connections in connections_df where END_ID matches a CUI pattern (including compound CUIs)\n",
    "cui_mask = connections_df[':END_ID'].str.contains(r'C\\d+', regex=True, na=False)\n",
    "connections_df.loc[cui_mask, ':END_ID'] = connections_df.loc[cui_mask, ':END_ID'].map(cui_to_uuid)\n",
    "\n",
    "# Save the modified files\n",
    "concept_df.to_csv(\"concept_updated.csv\", index=False)\n",
    "connections_df.to_csv(\"connections_updated.csv\", index=False)\n",
    "\n",
    "# Display sample of the changes\n",
    "print(\"\\nUpdated concept.csv sample:\")\n",
    "print(concept_df.head(3).to_string())\n",
    "print(\"\\nUpdated connections.csv sample:\")\n",
    "print(connections_df.head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for predication rows where subject_cui or object_cui doesn't match the C******* pattern\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the predication data\n",
    "predication_df = pd.read_csv(\"data/predication.csv\")\n",
    "\n",
    "# Define a function to check if a CUI follows the standard format\n",
    "def is_not_standard_cui(cui_str):\n",
    "    # Check if the string is None or NaN\n",
    "    if pd.isna(cui_str):\n",
    "        return True\n",
    "    \n",
    "    # Check if the string doesn't match the C******* pattern\n",
    "    # This will also catch compound CUIs with pipe separators\n",
    "    if not re.match(r'^C\\d+$', str(cui_str)):\n",
    "        # If it's a compound CUI with pipe separator, check each part\n",
    "        if '|' in str(cui_str):\n",
    "            parts = str(cui_str).split('|')\n",
    "            # If any part doesn't match C******* or is purely numeric, flag it\n",
    "            return any(not (re.match(r'^C\\d+$', part) or part.isdigit()) for part in parts)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Find rows where subject_cui doesn't match the pattern\n",
    "non_standard_subject = predication_df[predication_df['subject_cui:STRING'].apply(is_not_standard_cui)]\n",
    "\n",
    "# Find rows where object_cui doesn't match the pattern\n",
    "non_standard_object = predication_df[predication_df['object_cui:STRING'].apply(is_not_standard_cui)]\n",
    "\n",
    "# Display the results\n",
    "print(\"Rows where subject_cui doesn't match the C******* pattern:\")\n",
    "if len(non_standard_subject) > 0:\n",
    "    print(non_standard_subject)\n",
    "else:\n",
    "    print(\"No rows found with non-standard subject_cui format.\")\n",
    "\n",
    "print(\"\\nRows where object_cui doesn't match the C******* pattern:\")\n",
    "if len(non_standard_object) > 0:\n",
    "    print(non_standard_object)\n",
    "else:\n",
    "    print(\"No rows found with non-standard object_cui format.\")\n",
    "\n",
    "# Count of non-standard CUIs\n",
    "print(f\"\\nTotal rows with non-standard subject_cui: {len(non_standard_subject)}\")\n",
    "print(f\"Total rows with non-standard object_cui: {len(non_standard_object)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the predication data\n",
    "df = pd.read_csv(\"semmed_data/predication.csv\", \n",
    "                 names=predication_headers,\n",
    "                 encoding='ISO-8859-1',  # Add this parameter\n",
    "                 on_bad_lines='warn',    # Optionally handle bad lines\n",
    "                 na_values=['\\\\N'])      # Handle NULL values\n",
    "\n",
    "def is_non_standard_cui(cui):\n",
    "    # Handle NaN/None values\n",
    "    if pd.isna(cui):\n",
    "        return True\n",
    "        \n",
    "    cui_str = str(cui)\n",
    "    # Split for compound CUIs\n",
    "    cuis = cui_str.split('|')\n",
    "    \n",
    "    for single_cui in cuis:\n",
    "        # Check if it matches the standard C******* pattern\n",
    "        if not re.match(r'^C\\d+$', single_cui):\n",
    "            # If it's a pure number (like \"3075\" in compound CUIs), it's acceptable\n",
    "            if not single_cui.isdigit():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Find non-standard CUIs\n",
    "non_standard_subjects = df[df['SUBJECT_CUI'].apply(is_non_standard_cui)]\n",
    "non_standard_objects = df[df['OBJECT_CUI'].apply(is_non_standard_cui)]\n",
    "\n",
    "# Print summary\n",
    "print(\"=== Non-standard CUI Analysis ===\")\n",
    "print(f\"\\nTotal rows in dataset: {len(df)}\")\n",
    "print(f\"Rows with non-standard subject CUIs: {len(non_standard_subjects)}\")\n",
    "print(f\"Rows with non-standard object CUIs: {len(non_standard_objects)}\")\n",
    "\n",
    "# Show sample of non-standard entries\n",
    "print(\"\\nSample of non-standard subject CUIs:\")\n",
    "print(non_standard_subjects[['PREDICATION_ID', 'SUBJECT_CUI', 'SUBJECT_NAME']].head())\n",
    "\n",
    "print(\"\\nSample of non-standard object CUIs:\")\n",
    "print(non_standard_objects[['PREDICATION_ID', 'OBJECT_CUI', 'OBJECT_NAME']].head())\n",
    "\n",
    "# Get unique patterns of non-standard CUIs\n",
    "print(\"\\nUnique patterns of non-standard subject CUIs:\")\n",
    "print(non_standard_subjects['SUBJECT_CUI'].unique()[:10])\n",
    "\n",
    "print(\"\\nUnique patterns of non-standard object CUIs:\")\n",
    "print(non_standard_objects['OBJECT_CUI'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Output all non-standard CUIs to a CSV file\n",
    "output_file = \"non_standard_cuis.csv\"\n",
    "\n",
    "# Get all unique non-standard CUIs\n",
    "subject_cuis = non_standard_subjects['SUBJECT_CUI'].dropna().unique().tolist()\n",
    "object_cuis = non_standard_objects['OBJECT_CUI'].dropna().unique().tolist()\n",
    "\n",
    "# Combine all unique non-standard CUIs\n",
    "all_non_standard_cuis = list(set(subject_cuis + object_cuis))\n",
    "\n",
    "# Write to CSV file (one CUI per line)\n",
    "with open(output_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['cui'])  # Header\n",
    "    for cui in all_non_standard_cuis:\n",
    "        writer.writerow([cui])\n",
    "\n",
    "print(f\"All {len(all_non_standard_cuis)} unique non-standard CUIs have been written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a specific CUI in the predication.csv file\n",
    "import pandas as pd\n",
    "\n",
    "# Define the CUI to search for\n",
    "search_cui = \"7523\"\n",
    "\n",
    "# Read the predication.csv file\n",
    "# Note: Adjust the file path if needed\n",
    "# First try to read from the original predication.csv file\n",
    "pred_df = pd.read_csv(\"semmed_data/predication.csv\", \n",
    "                names=predication_headers,\n",
    "                encoding='ISO-8859-1',  # Add this parameter\n",
    "                on_bad_lines='warn',    # Optionally handle bad lines\n",
    "                na_values=['\\\\N']) \n",
    "\n",
    "# Search for the CUI in either subject or object columns\n",
    "matching_rows = pred_df[(pred_df['SUBJECT_CUI'].astype(str).str.contains(search_cui)) | \n",
    "                        (pred_df['OBJECT_CUI'].astype(str).str.contains(search_cui))]\n",
    "\n",
    "if len(matching_rows) > 0:\n",
    "    print(f\"Found {len(matching_rows)} rows containing CUI '{search_cui}':\")\n",
    "    print(matching_rows)\n",
    "else:\n",
    "    print(f\"No rows found containing CUI '{search_cui}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def process_predication_csv():\n",
    "    input_path = 'data/predication.csv'\n",
    "    output_path = 'data/predication_prefixed.csv'\n",
    "    \n",
    "    # Read the predication header to get column names\n",
    "    with open('data/predication_header.csv', 'r') as header_file:\n",
    "        header_reader = csv.reader(header_file)\n",
    "        headers = next(header_reader)\n",
    "    \n",
    "    # Process the predication file\n",
    "    print(\"Processing predication.csv to add K prefix to non-C CUIs...\")\n",
    "    \n",
    "    # Read the file in chunks to handle large files efficiently\n",
    "    chunk_size = 100000\n",
    "    chunks_processed = 0\n",
    "    examples_shown = 0\n",
    "    \n",
    "    with open(output_path, 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(headers)  # Write header row\n",
    "        \n",
    "        for chunk in pd.read_csv(input_path, names=headers, chunksize=chunk_size):\n",
    "            modified_rows = 0\n",
    "            \n",
    "            for _, row in chunk.iterrows():\n",
    "                row_data = row.tolist()\n",
    "                \n",
    "                # Check and modify SUBJECT_CUI if it doesn't start with 'C'\n",
    "                if isinstance(row_data[4], str) and not row_data[4].startswith('C'):\n",
    "                    row_data[4] = f'K{row_data[4]}'\n",
    "                    modified_rows += 1\n",
    "                \n",
    "                # Check and modify OBJECT_CUI if it doesn't start with 'C'\n",
    "                if isinstance(row_data[5], str) and not row_data[5].startswith('C'):\n",
    "                    row_data[5] = f'K{row_data[5]}'\n",
    "                    modified_rows += 1\n",
    "                \n",
    "                writer.writerow(row_data)\n",
    "                \n",
    "                # Print a few examples of modified rows\n",
    "                if modified_rows > 0 and examples_shown < 5 and (row_data[4].startswith('K') or row_data[5].startswith('K')):\n",
    "                    print(f\"Modified row: {row_data}\")\n",
    "                    examples_shown += 1\n",
    "            \n",
    "            chunks_processed += 1\n",
    "            print(f\"Processed chunk {chunks_processed}, modified {modified_rows} CUIs\")\n",
    "    \n",
    "    print(f\"Processing complete. Check {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_predication_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df = pd.read_csv(\"data/concept_prefixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_df.to_csv(\"concept.csv\", index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to add a column\n",
    "import csv\n",
    "\n",
    "def add_concept_column(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r', newline='', encoding='ISO-8859-1') as infile, \\\n",
    "             open(output_file, 'w', newline='', encoding='ISO-8859-1') as outfile:\n",
    "            \n",
    "            reader = csv.reader(infile)\n",
    "            writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "            \n",
    "            # Process each row and add 'Concept'\n",
    "            for row in reader:\n",
    "                row.append(\"Sentence\")\n",
    "                writer.writerow(row)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Usage\n",
    "add_concept_column('data/sentence.csv', 'sentence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility script to restore quotes on a CSV\n",
    "import pandas as pd\n",
    "df = pd.read_csv('semmed_data/sentence.csv', on_bad_lines='skip'); df.to_csv('output.csv', quoting=1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load only the first three rows from the predication CSV file\n",
    "predication_df = pd.read_csv('data/predication.csv', header=None, usecols=[0, 1, 2], nrows=3)\n",
    "sentence_df = pd.read_csv('semmed_data/sentence.csv', header=None, usecols=[0])\n",
    "citations_df = pd.read_csv('semmed_data/citations.csv', header=None, usecols=[0, 1])\n",
    "\n",
    "\n",
    "# Find matching citations and sentences for the first three predications\n",
    "for index, row in predication_df.iterrows():\n",
    "    sentence_id = row[1]\n",
    "    pmid = row[2]\n",
    "    \n",
    "    # Find the matching sentence\n",
    "    matching_sentence = sentence_df[sentence_df[0] == sentence_id]\n",
    "    \n",
    "    # Find the matching citation\n",
    "    matching_citation = citations_df[citations_df[0] == pmid]\n",
    "    \n",
    "    print(f\"Predication ID: {row}\")\n",
    "    print(\"Matching Sentence:\")\n",
    "    print(matching_sentence)\n",
    "    print(\"Matching Citation:\")\n",
    "    print(matching_citation)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "predication_file = 'data/predication.csv'\n",
    "predication_headers = pd.read_csv('data/predication_header.csv').columns.tolist()\n",
    "citation_file = 'semmed_data/citations.csv'\n",
    "citation_headers = pd.read_csv('semmed_data/citations_header.csv').columns.tolist()\n",
    "connections_file = 'connections.csv'\n",
    "\n",
    "# Read predication and citation data in chunks\n",
    "chunk_size = 10000  # Adjust based on your memory capacity\n",
    "predication_chunks = pd.read_csv(predication_file, names=predication_headers, chunksize=chunk_size)\n",
    "citation_chunks = pd.read_csv(citation_file, names=citation_headers, chunksize=chunk_size)\n",
    "\n",
    "# Initialize an empty DataFrame for new connections\n",
    "new_connections = pd.DataFrame(columns=['StartNode', 'EndNode', ':TYPE'])\n",
    "\n",
    "# Process each chunk\n",
    "for pred_chunk in predication_chunks:\n",
    "    for cit_chunk in citation_chunks:\n",
    "        # Merge on 'PMID'\n",
    "        merged = pd.merge(pred_chunk, cit_chunk, on='PMID', how='inner')\n",
    "        \n",
    "        # Create new connections\n",
    "        connections = pd.DataFrame({\n",
    "            'StartNode': merged['PREDICATION_ID:ID'],  # Assuming 'PredicationID' is the identifier\n",
    "            'EndNode': merged['PMID'],       # Assuming 'CitationID' is the identifier\n",
    "            ':TYPE': 'REF'\n",
    "        })\n",
    "        \n",
    "        # Append to new_connections DataFrame\n",
    "        new_connections = pd.concat([new_connections, connections], ignore_index=True)\n",
    "\n",
    "# Write new connections to the connections.csv\n",
    "new_connections.to_csv(connections_file, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "citations_file = 'data/citations.csv'\n",
    "sentences_file = 'data/sentence.csv'\n",
    "citations_header_file = 'data/citations_header.csv'\n",
    "sentences_header_file = 'data/sentences_header.csv'\n",
    "\n",
    "# Read headers\n",
    "citations_header = pd.read_csv(citations_header_file).columns.tolist()\n",
    "sentences_header = pd.read_csv(sentences_header_file).columns.tolist()\n",
    "\n",
    "# Initialize an empty list to store connections\n",
    "connections = []\n",
    "\n",
    "# Process citations in chunks\n",
    "citations_chunks = pd.read_csv(citations_file, names=citations_header,on_bad_lines='skip', chunksize=10000)\n",
    "\n",
    "# Create a dictionary to store citation PMIDs for quick lookup\n",
    "citation_pmids = set()\n",
    "for chunk in citations_chunks:\n",
    "    citation_pmids.update(chunk['PMID:ID'].dropna().unique())\n",
    "\n",
    "# Process sentences in chunks\n",
    "sentences_chunks = pd.read_csv(sentences_file, names=sentences_header, on_bad_lines='skip', chunksize=10000)\n",
    "\n",
    "# Iterate over each chunk of sentences\n",
    "for sentences_chunk in sentences_chunks:\n",
    "    for index, sentence_row in sentences_chunk.iterrows():\n",
    "        pmid = sentence_row['PMID']\n",
    "        # Check if the citation exists for the given PMID\n",
    "        if pd.notna(pmid) and pmid in citation_pmids:\n",
    "            # Create a connection entry\n",
    "            connection = {\n",
    "                'SentenceID': sentence_row['id:ID(Sentence)'],\n",
    "                'CitationID': pmid,  # Using PMID as CitationID since no specific CitationID is provided\n",
    "                ':TYPE': 'IS_IN'\n",
    "            }\n",
    "            connections.append(connection)\n",
    "\n",
    "# Convert the connections list to a DataFrame\n",
    "connections_df = pd.DataFrame(connections)\n",
    "\n",
    "# Write the connections to a new CSV file\n",
    "connections_df.to_csv('connections_1.csv', index=False)\n",
    "\n",
    "print(\"Connections have been successfully written to 'connections_1.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility script to restore quotes on a CSV in chunks\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Process the file in chunks\n",
    "csv_reader = pd.read_csv('data/connections_1.csv', on_bad_lines='skip', chunksize=chunk_size)\n",
    "\n",
    "# Open the output file\n",
    "with open('output.csv', 'w', newline='') as f_out:\n",
    "    # Initialize a flag to track if we've written the header\n",
    "    header_written = False\n",
    "    \n",
    "    # Process each chunk\n",
    "    for chunk in csv_reader:\n",
    "        # For the first chunk, write the header with quotes\n",
    "        if not header_written:\n",
    "            chunk.to_csv(f_out, quoting=csv.QUOTE_ALL, index=False)\n",
    "            header_written = True\n",
    "        else:\n",
    "            # For subsequent chunks, append without header\n",
    "            chunk.to_csv(f_out, quoting=csv.QUOTE_ALL, index=False, header=False, mode='a')\n",
    "            \n",
    "print(\"CSV processing completed with quotes restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update ID for sentence since there are overlaps in the IDs. Prefix with S\n",
    "import csv\n",
    "\n",
    "def prefix_first_field(input_path, output_path, prefix='X'):\n",
    "    with open(input_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile, quoting=csv.QUOTE_ALL)\n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row:  # skip empty rows\n",
    "                row[0] = prefix + row[0]\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Example usage:\n",
    "prefix_first_field('data/sentence.csv', 'output.csv', prefix='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update ID in predication since they refer to the sentence IDs. \n",
    "import csv\n",
    "\n",
    "def prefix_first_field(input_path, output_path, prefix='X'):\n",
    "    with open(input_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile, quoting=csv.QUOTE_ALL)\n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row:  # skip empty rows\n",
    "                row[1] = prefix + row[1]\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Example usage:\n",
    "prefix_first_field('data/predication.csv', 'output_predication.csv', prefix='S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update PMID in the same was as above\n",
    "# Update ID in predication since they refer to the sentence IDs. \n",
    "import csv\n",
    "\n",
    "def prefix_first_field(input_path, output_path, prefix='X'):\n",
    "    with open(input_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile, quoting=csv.QUOTE_ALL)\n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row:  # skip empty rows\n",
    "                row[0] = prefix + row[0]\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Example usage:\n",
    "prefix_first_field('data/citations.csv', 'output_citations.csv', prefix='P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update PMID in the same was as above\n",
    "# for sentence and predication\n",
    "import csv\n",
    "\n",
    "def prefix_field(input_path, output_path, field_index=0, prefix='X'):\n",
    "    with open(input_path, mode='r', newline='', encoding='ISO-8859-1') as infile, \\\n",
    "         open(output_path, mode='w', newline='', encoding='ISO-8859-1') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile, quoting=csv.QUOTE_ALL)\n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row and len(row) > field_index:  # skip empty rows and check field exists\n",
    "                row[field_index] = prefix + row[field_index]\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Example usage:\n",
    "prefix_field('data/sentence.csv', 'output_sentence.csv', 0, prefix='S')\n",
    "prefix_field('data/predication.csv', 'output_predication.csv', 2, prefix='P')\n",
    "# prefix_field('data/connections_1.csv', \"connections_1_1\", 1, \"P\")\n",
    "# prefix_field('data/connections_2.csv', \"connections_2_1\", 1, \"P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove commas from the 6th column (index 5) in sentence.csv\n",
    "import csv\n",
    "\n",
    "def remove_commas_from_column(input_path, output_path, column_index=5):\n",
    "    with open(input_path, mode='r', newline='', encoding='ISO-8859-1') as infile, \\\n",
    "         open(output_path, mode='w', newline='', encoding='ISO-8859-1') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile, quoting=csv.QUOTE_ALL)\n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row and len(row) > column_index:  # Skip empty rows and check column exists\n",
    "                # Remove commas from the specified column\n",
    "                row[column_index] = row[column_index].replace(',', '')\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Execute the function to remove commas from the 6th column (index 5)\n",
    "remove_commas_from_column('data/sentence.csv', 'output_sentence_no_commas.csv')\n",
    "print(\"Commas removed from the 6th column in sentence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the structure of data/sentence.csv\n",
    "import csv\n",
    "\n",
    "def verify_sentence_csv_structure(file_path):\n",
    "    expected_columns = 9  # Based on: id:ID(Sentence),PMID,type,number,sent_start_index,sent_end_index,section_header,normalized_section_header,sentence,:LABEL\n",
    "    \n",
    "    with open(file_path, mode='r', newline='', encoding='ISO-8859-1') as file:\n",
    "        reader = csv.reader(file, quoting=csv.QUOTE_ALL)\n",
    "        \n",
    "        # Check all rows to ensure consistent structure\n",
    "        row_count = 0\n",
    "        inconsistent_rows = []\n",
    "        \n",
    "        for i, row in enumerate(reader):\n",
    "            row_count += 1\n",
    "            if len(row) != expected_columns:\n",
    "                inconsistent_rows.append((i+1, len(row)))\n",
    "                \n",
    "        if inconsistent_rows:\n",
    "            print(f\"Found {len(inconsistent_rows)} rows with incorrect column count:\")\n",
    "            for row_num, col_count in inconsistent_rows[:10]:  # Show first 10 problematic rows\n",
    "                print(f\"Row {row_num} has {col_count} columns instead of the expected {expected_columns}\")\n",
    "            if len(inconsistent_rows) > 10:\n",
    "                print(f\"... and {len(inconsistent_rows) - 10} more inconsistent rows\")\n",
    "            return False\n",
    "                \n",
    "        print(f\"Verified: All {row_count} rows in {file_path} have the expected {expected_columns} columns\")\n",
    "        return True\n",
    "\n",
    "# Execute the verification\n",
    "verify_sentence_csv_structure('semmed_data/sentence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence is messed up so I'm trying to clean it:\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def validate_and_clean_csv(input_file, output_file, expected_cols=10):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        \n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        problematic_rows_count = 0\n",
    "        total_rows = 0\n",
    "        \n",
    "        for line_num, line in enumerate(infile, 1):\n",
    "            try:\n",
    "                # Parse the line as CSV\n",
    "                row = list(csv.reader([line]))[0]\n",
    "                total_rows += 1\n",
    "                \n",
    "                if len(row) == expected_cols:\n",
    "                    # Row has correct number of columns, write as is\n",
    "                    writer.writerow(row)\n",
    "                else:\n",
    "                    problematic_rows_count += 1\n",
    "                    cleaned_row = clean_problematic_row(row)\n",
    "                    writer.writerow(cleaned_row)\n",
    "                \n",
    "                # Progress update every 10000 rows\n",
    "                if total_rows % 10000 == 0:\n",
    "                    print(f\"Processed {total_rows} rows. Found {problematic_rows_count} problematic rows so far...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error on line {line_num}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Total rows processed: {total_rows}\")\n",
    "        print(f\"Total problematic rows fixed: {problematic_rows_count}\")\n",
    "\n",
    "def clean_problematic_row(row):\n",
    "    \"\"\"Clean a problematic row by finding the correct column boundaries.\"\"\"\n",
    "    \n",
    "    # First 5 columns are always correct\n",
    "    prefix = row[:5]\n",
    "    \n",
    "    # Find the last three columns (should be [\"\", \"\", \"Sentence\"])\n",
    "    # and the number that comes before them\n",
    "    position = None\n",
    "    text_parts = []\n",
    "    \n",
    "    # Work backwards from the end\n",
    "    for i in range(len(row)-1, -1, -1):\n",
    "        if row[i] == \"Sentence\":\n",
    "            # Found the end marker\n",
    "            if i >= 3 and row[i-1] == \"\" and row[i-2] == \"\":\n",
    "                # Found the empty strings\n",
    "                if i >= 4 and row[i-3].strip().isdigit():\n",
    "                    # Found the position number\n",
    "                    position = row[i-3]\n",
    "                    text_parts = row[5:i-3]  # Everything between prefix and position\n",
    "                    break\n",
    "    \n",
    "    if not position:\n",
    "        # Fallback: look for first number after position 5\n",
    "        for i in range(5, len(row)):\n",
    "            if row[i].strip().isdigit():\n",
    "                position = row[i]\n",
    "                text_parts = row[5:i]\n",
    "                break\n",
    "    \n",
    "    # Clean and combine the text parts\n",
    "    text_content = clean_text_field(\" \".join(text_parts))\n",
    "    \n",
    "    # Construct the final row\n",
    "    return prefix + [text_content, position, \"\", \"\", \"Sentence\"]\n",
    "\n",
    "def clean_text_field(text):\n",
    "    \"\"\"Clean the text content by handling quotes and other issues.\"\"\"\n",
    "    \n",
    "    # Remove escaped quotes patterns\n",
    "    text = re.sub(r'\\\\+[\"\\']', '', text)\n",
    "    \n",
    "    # Remove standalone quotes that aren't part of actual quotations\n",
    "    text = re.sub(r'(?<=[^\"])\"(?=[^\"])', '', text)\n",
    "    \n",
    "    # Remove multiple quotes at the end of the text\n",
    "    text = re.sub(r'\"{2,}$', '', text)\n",
    "    \n",
    "    # Fix spaces around punctuation\n",
    "    text = re.sub(r'\\s+([.,)])', r'\\1', text)\n",
    "    text = re.sub(r'(\\()\\s+', r'\\1', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove any trailing quotes or spaces\n",
    "    text = text.strip('\" ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "input_file = 'data/sentence.csv'\n",
    "output_file = 'sentence.csv'\n",
    "validate_and_clean_csv(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first row (header) from connections_1.csv without using pandas\n",
    "import csv\n",
    "\n",
    "# Path to the file\n",
    "connections_file = 'data/connections_1.csv'\n",
    "\n",
    "# Read all rows from the CSV file\n",
    "with open(connections_file, 'r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)[1:]  # Skip the first row (header)\n",
    "\n",
    "# Write back all rows except the header\n",
    "with open(connections_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"First row deleted from {connections_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine multiple connection files into one\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def combine_csv_files(input_files, output_file, buffer_size=10000):\n",
    "    \"\"\"\n",
    "    Combine multiple CSV files into a single CSV file with memory efficiency.\n",
    "    Assumes all files have the same structure.\n",
    "    \n",
    "    Args:\n",
    "        input_files: List of input CSV file paths\n",
    "        output_file: Path to the output combined CSV file\n",
    "        buffer_size: Number of rows to process at once to reduce memory usage\n",
    "    \"\"\"\n",
    "    # Check if input files exist\n",
    "    valid_input_files = []\n",
    "    for file in input_files:\n",
    "        if not os.path.exists(file):\n",
    "            print(f\"Warning: File {file} does not exist. Skipping.\")\n",
    "        else:\n",
    "            valid_input_files.append(file)\n",
    "    \n",
    "    if not valid_input_files:\n",
    "        print(\"No valid input files found.\")\n",
    "        return\n",
    "    \n",
    "    # Get total line count for progress tracking\n",
    "    total_lines = 0\n",
    "    file_row_counts = {}\n",
    "    \n",
    "    print(\"Counting rows in input files...\")\n",
    "    for file in valid_input_files:\n",
    "        with open(file, 'r', newline='') as f:\n",
    "            row_count = sum(1 for _ in f)\n",
    "            file_row_counts[file] = row_count\n",
    "            total_lines += row_count\n",
    "    \n",
    "    # Process files in streaming fashion to avoid loading everything into memory\n",
    "    with open(output_file, 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Process each file\n",
    "        processed_rows = 0\n",
    "        is_first_file = True\n",
    "        \n",
    "        for file in valid_input_files:\n",
    "            print(f\"Processing {file}...\")\n",
    "            with open(file, 'r', newline='') as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                \n",
    "                # Write header only from the first file\n",
    "                if is_first_file:\n",
    "                    header = next(reader)\n",
    "                    writer.writerow(header)\n",
    "                    processed_rows += 1\n",
    "                    is_first_file = False\n",
    "                else:\n",
    "                    # Skip header for subsequent files\n",
    "                    next(reader, None)\n",
    "                \n",
    "                # Process the file in chunks to save memory\n",
    "                buffer = []\n",
    "                for row in tqdm(reader, total=file_row_counts[file]-1, desc=f\"Processing {os.path.basename(file)}\"):\n",
    "                    buffer.append(row)\n",
    "                    \n",
    "                    # Write buffer when it reaches the specified size\n",
    "                    if len(buffer) >= buffer_size:\n",
    "                        writer.writerows(buffer)\n",
    "                        processed_rows += len(buffer)\n",
    "                        buffer = []\n",
    "                \n",
    "                # Write any remaining rows in the buffer\n",
    "                if buffer:\n",
    "                    writer.writerows(buffer)\n",
    "                    processed_rows += len(buffer)\n",
    "    \n",
    "    # Print row counts for each file\n",
    "    print(\"\\nRow counts for each file:\")\n",
    "    for file, count in file_row_counts.items():\n",
    "        print(f\"  {file}: {count} rows\")\n",
    "    \n",
    "    print(f\"\\nCombined {len(valid_input_files)} files into {output_file}\")\n",
    "    print(f\"Total rows in combined file: {processed_rows}\")\n",
    "\n",
    "# Files to combine\n",
    "input_files = [\n",
    "    'data/connections_1.csv',\n",
    "    'data/connections_2.csv',\n",
    "    'data/connections.csv'\n",
    "]\n",
    "\n",
    "# Output file\n",
    "output_file = 'data/combined_connections.csv'\n",
    "\n",
    "# Combine the files\n",
    "combine_csv_files(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_connections_with_sorting():\n",
    "    input_file = 'data/combined_connections.csv'\n",
    "    output_file = 'data/deduplicated_connections.csv'\n",
    "    chunk_size = 1000000  # Adjust based on available memory\n",
    "    \n",
    "    def sort_chunk(chunk):\n",
    "        return sorted(chunk, key=itemgetter(0, 1, 2))\n",
    "    \n",
    "    print(\"Processing and sorting connections...\")\n",
    "    temp_files = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    # First, count total lines for progress bar\n",
    "    with open(input_file, 'r') as f:\n",
    "        total_lines = sum(1 for _ in f) - 1  # subtract header\n",
    "    \n",
    "    # Read and sort in chunks\n",
    "    with open(input_file, 'r', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        \n",
    "        for i, row in tqdm(enumerate(reader), total=total_lines, desc=\"Processing rows\"):\n",
    "            if len(row) >= 3:\n",
    "                current_chunk.append((row[0], row[1], row[2]))\n",
    "            \n",
    "            if len(current_chunk) >= chunk_size:\n",
    "                # Sort chunk and write to temporary file\n",
    "                temp_file = NamedTemporaryFile(delete=False, mode='w', newline='')\n",
    "                writer = csv.writer(temp_file)\n",
    "                writer.writerows(sort_chunk(current_chunk))\n",
    "                temp_file.close()\n",
    "                temp_files.append(temp_file.name)\n",
    "                current_chunk = []\n",
    "    \n",
    "    # Handle last chunk if any\n",
    "    if current_chunk:\n",
    "        temp_file = NamedTemporaryFile(delete=False, mode='w', newline='')\n",
    "        writer = csv.writer(temp_file)\n",
    "        writer.writerows(sort_chunk(current_chunk))\n",
    "        temp_file.close()\n",
    "        temp_files.append(temp_file.name)\n",
    "    \n",
    "    print(f\"\\nCreated {len(temp_files)} temporary sorted files\")\n",
    "    \n",
    "    # Merge sorted chunks and count frequencies\n",
    "    print(\"\\nMerging and counting frequencies...\")\n",
    "    with open(output_file, 'w', newline='') as out_f:\n",
    "        writer = csv.writer(out_f)\n",
    "        writer.writerow([':START_ID', ':END_ID', ':TYPE', 'frequency'])\n",
    "        \n",
    "        # Initialize readers and current values\n",
    "        files = []\n",
    "        readers = []\n",
    "        current_values = []\n",
    "        \n",
    "        try:\n",
    "            # Open all temp files and get their first values\n",
    "            for temp_file in temp_files:\n",
    "                f = open(temp_file, 'r', newline='')\n",
    "                files.append(f)\n",
    "                reader = csv.reader(f)\n",
    "                first_row = next(reader, None)\n",
    "                if first_row:\n",
    "                    readers.append(reader)\n",
    "                    current_values.append([first_row, reader, 1])  # [row, reader, count]\n",
    "            \n",
    "            while current_values:\n",
    "                # Sort current values to find duplicates across files\n",
    "                current_values.sort(key=lambda x: x[0])\n",
    "                \n",
    "                # Take the first value\n",
    "                current_row = current_values[0][0]\n",
    "                current_count = current_values[0][2]\n",
    "                \n",
    "                # Remove the processed value\n",
    "                reader = current_values[0][1]\n",
    "                next_row = next(reader, None)\n",
    "                \n",
    "                if next_row:\n",
    "                    if next_row == current_row:\n",
    "                        # If next row is the same, increment count and keep in list\n",
    "                        current_values[0] = [next_row, reader, current_count + 1]\n",
    "                    else:\n",
    "                        # If next row is different, write current count and update with new row\n",
    "                        writer.writerow(current_row + [current_count])\n",
    "                        current_values[0] = [next_row, reader, 1]\n",
    "                else:\n",
    "                    # If no more rows in this reader, write current count and remove from list\n",
    "                    writer.writerow(current_row + [current_count])\n",
    "                    current_values.pop(0)\n",
    "                \n",
    "                # Re-sort remaining values if needed\n",
    "                current_values.sort(key=lambda x: x[0])\n",
    "                \n",
    "                # Combine counts for identical rows across different files\n",
    "                i = 1\n",
    "                while i < len(current_values):\n",
    "                    if current_values[i][0] == current_values[0][0]:\n",
    "                        current_values[0][2] += current_values[i][2]\n",
    "                        # Get next row from the reader we're combining\n",
    "                        reader = current_values[i][1]\n",
    "                        next_row = next(reader, None)\n",
    "                        if next_row:\n",
    "                            current_values[i][0] = next_row\n",
    "                            current_values[i][2] = 1\n",
    "                        else:\n",
    "                            current_values.pop(i)\n",
    "                            continue\n",
    "                    i += 1\n",
    "                \n",
    "        finally:\n",
    "            # Clean up\n",
    "            for f in files:\n",
    "                f.close()\n",
    "            for temp_file in temp_files:\n",
    "                try:\n",
    "                    os.unlink(temp_file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not delete temporary file {temp_file}: {e}\")\n",
    "    \n",
    "    print(f\"Completed. Deduplicated connections saved to {output_file}\")\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        count_connections_with_sorting()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding frequency column to data/connections_1.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 263129118/263129118 [02:46<00:00, 1579639.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed. Output saved to data/connections_with_frequency_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def add_frequency_column(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Add a 'frequency' column to a CSV file and set all values to 1.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file\n",
    "        output_file (str): Path to save the output CSV file\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Adding frequency column to {input_file}...\")\n",
    "    \n",
    "    # Count total lines for progress bar\n",
    "    with open(input_file, 'r', newline='') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    with open(input_file, 'r', newline='') as infile, \\\n",
    "         open(output_file, 'w', newline='') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Read and modify header\n",
    "        header = next(reader)\n",
    "        writer.writerow(header + ['frequency'])\n",
    "        \n",
    "        # Process each row with progress bar\n",
    "        with tqdm(total=total_lines-1, desc=\"Processing rows\") as pbar:\n",
    "            for row in reader:\n",
    "                writer.writerow(row + ['1'])\n",
    "                pbar.update(1)\n",
    "    \n",
    "    print(f\"Completed. Output saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"data/connections_1.csv\"\n",
    "    output_file = \"data/connections_with_frequency_1.csv\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Error: Input file {input_file} not found.\")\n",
    "    else:\n",
    "        add_frequency_column(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
